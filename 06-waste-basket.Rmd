# Waste basket

## Repeated measures and the independence assumption

Two events $A$ and $B$ are independent if the occurrence (or absence) of neither is unaffected by
the occurrence (or absence) of the other. That is, the following must hold:

$$P(A \mid B) = P(A) \text{ and } P(B \mid A) = P(B) $$

A shorter (by in my view less intuitive) formulation is this:

$$P(A \cap B) = P(A)P(B)$$


## Contrast coding and interactions

NOTE: Contrast coding only applies when categorical predictors are used. With continuous predictors, this is not needed.

sum-coding has important advantages when interactions are present

To check the contrasts that R assigns to a factor, use the `contrasts` command. To change the contrast coding, we can do something like this:

```{r contrasts}
x <- factor(c("A", "B"))
# check the contrasts
contrasts(x)

# change to sum-conding
contrasts(x) <- contr.sum(2)
contrasts(x)

# go back to treatment coding
contrasts(x) <- contr.treatment(2)
contrasts(x)
```

(There are other ways of changing this. Most packages offer a contrast coding argument in the relevant call to the regression function. But this one is much more explicit.)

## Effect size

```{r effsize}
library(effsize)
diamonds_sub %>%
    cohen.d(price ~ color, data = .)
```

$$d = \frac{\bar{x}_1-\bar{x}_2}{s}$$

```{r p_effsize}
diamonds_sub %>%
    ggplot(aes(x = price, fill = color, color = color)) +
    geom_density(alpha = .5)
```

## Power calculations for mixed models

While we can often estimate a studies power a priori for simple tests, for mixed models this is not so easy. One way to do with is via simulations. 

The `simr` package might be used to do this.

## What random effects should I specify?

Approaches differ. 

Some people argue for parsimonious models: @matuschek2017type.

Others prefer the most maximal random effects structure: @barr2013randomeffects.

The most common way in the field currently seems to be the second one, but I am not so sure this is right.

## Differences between linear and generalized linear models

**Linear models (LM)** describe a linear relationship between the response variable and the predictors. The linear relationship is found in the raw data. The residuals are captured via the epsilon in the model model formula.

$$y_i = \beta_0 + \beta_1x_i + \epsilon$$

The probability distribution of the response variable is assumed to be the normal distribution.

Assumes variance homogeneity.

**Generalized linear models (GLN)** describe a relationship between the mean of the response variable and the predictors. The linear approximation of the mean is established via a link function ($f$ below) -- logistic regression used log-odds, Poisson regression uses natural log. Since only conditional means are modelled, there is no error term: 

$$f(\mu_{y|x}) = \beta_0 + \beta_1x_i$$

The probability distribution of the response variable varies by the kind of family: Poisson, binomial. etc.

Variance homogeneity is impossible for Poisson regression, it is always heteroscedastic: Since the Poisson distribution depends only on the a single parameter, as the mean increases, so does the variance. 

Because of the single parameter that the poisson distribution estimates, its assumption is that the mean and the variance of the response are equal. If this is not the case, then the model is overdispersed, and should not be used to calculate significance, as it underestimates the standard errors and thus becomes anticonservative. Alternative models in these situations are the quasipoisson model, where the standard errors are articificially inflated to correct for the assumption violation or the negative binomial model, which has an additional parameter. But note that negative binomial models assume that mean and variance are not equal, so they shoul not be used when the assumptions of the Poisson regression are met.
