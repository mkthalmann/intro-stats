[["index.html", "Corpus Annotation and Data Analysis Equinox School: Introduction to Statistics 1 Welcome 1.1 Prerequisites 1.2 Get in touch Session Info", " Corpus Annotation and Data Analysis Equinox School: Introduction to Statistics Maik Thalmann maik.thalmann@gmail.com September 2022 1 Welcome Hey everyone, if you’re looking at this, I assume that you signed up for the CAnDA equinox school in Göttingen in September 2022 and want to attend the Introduction to Statistics course. Here, I will cover some tools and concepts that are going to be instrumental in making sure that you get the most out of this class. I know that it is not ideal to have an introductory class that assumes basic familiarity with some material already, but unfortunately we do not have an entire semester together, but only one short week. 1.1 Prerequisites Because our schedule is quite tight and the relevant material quite expansive, I will have to presuppose some familiarity with statistics. Below, I will briefly summarize what these requirements are (in the form of questions) and, if you do not yet feel comfortable with them, give some pointers on where to change that. 1.1.1 R As you might have guessed, our technical analysis tool will be R. Because an introduction to R would be a class in and of itself, it would be beneficial for all attendees of the class to at least have some basic knowledge – which includes having R and the editor of your choosing installed before the first session of the course. Here is a step-by-step installation guide. The relevant skills include: What is an R script? How do I create and execute one? Which program should I use for my R environment, i.e., for editing R scripts, viewing data and plots, and for running statistical analyses. The popular choice here is undoubtedly RStudio, but you may also use Visual Studio Code and set it up for R if you’re more comfortable with that. How do I load my data (.csv, .xlsx, .txt file) into R? How do I use external packages to expand on the capabilities of base R? (That is, how do I install the packages, and how do I load them in my current R session?) What are factors and integers and how do I switch between them in R? What is the $ operator in R? How do I manipulate a data frame (add new columns, change existing columns, etc.)? How do I write my own functions in R? For example a function that takes a numerical argument and returns its square. As an aside, in case I do show code for data manipulation or plots, I will mostly rely on the packages in the so-called tidyverse, a collection of R packages. While I do not consider familiarity with all of these packages essential, they are important (and often a time saver) independently of this class if you want to use R for your own data analysis or data visualization projects. The packages I will most heavily rely on are dplyr for data wrangling and ggplot2 for visualization purposes. If you know German (and prefer it over English resources), I have my own website to offer you as a way of (re)gaining familiarity with R. Sessions 1 through 6 should form a quite thorough background (with some skippable material). Otherwise, I can recommend the relevant chapters in Gries (2013) or Winter (2020), which provide a gentle introduction to using R. If you would like to have a look at other resources, just google around, there are plenty of introductions, both as text books (often freely available online) and as videos. Since you will not need any in-depth knowledge of R, anything that gets you to a place where you’re comfortable typing and reading commands should be enough. 1.1.2 Statistics As announced in the program for the class, I will, again for reasons of time, have to ask you to know your way around two widely used statistical tests and some basic notions of inferential statistics, detailed below. I am, of course, happy to answer questions during class and the practice sessions, but if you’re not as confident with the topics below, I would advise to do some prepatory reading to get the most out of the class (and the one in the second week of the summer school). What do the following terms mean: Mean, median, variance, and standard deviation? What is the \\(t\\)-test? What does the output of the t.test() command in R mean – see below? # load the tidyverse library(tidyverse) # subset the data to only have two colors diamond_sub &lt;- diamonds %&gt;% filter(color %in% c(&quot;E&quot;, &quot;J&quot;)) # show the first few rows of the data set head(diamond_sub) ## # A tibble: 6 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 5 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 6 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 # perform a t-test t.test(diamond_sub$price ~ diamond_sub$color, paired = FALSE) ## ## Welch Two Sample t-test ## ## data: diamond_sub$price by diamond_sub$color ## t = -24.8811, df = 3766.32, p-value &lt; 0.000000000000000222 ## alternative hypothesis: true difference in means between group E and group J is not equal to 0 ## 95 percent confidence interval: ## -2424.1311 -2070.0000 ## sample estimates: ## mean in group E mean in group J ## 3076.7525 5323.8180 In which scenarios is the \\(t\\)-test applicable and when is it not (scale levels, assumptions of the \\(t\\)-test, etc.)? What is the effect of setting the paired argument in the R command of t.test to TRUE? What is the \\(\\chi^2\\) test? Why does it find so much use in corpus linguistics compared to the \\(t\\)-test? What does the output below mean? # get the frequencies for both diamond colors in the data set (color_frequencies &lt;- diamond_sub %&gt;% group_by(color) %&gt;% summarise(frequency = n()) ) ## # A tibble: 2 × 2 ## color frequency ## &lt;ord&gt; &lt;int&gt; ## 1 E 9797 ## 2 J 2808 # perform the chi^2 test color_frequencies %&gt;% select(frequency) %&gt;% chisq.test() ## ## Chi-squared test for given probabilities ## ## data: . ## X-squared = 3875.14, df = 1, p-value &lt; 0.000000000000000222 What are proper and improper interpretations of a \\(p\\)-value? What is statistical significance? To brush up on statistics, you can also read (the relevant chapters in) Gries (2013) Alternatively, I recommend Vasishth and Broe (2010) up to (and including) chapter 3 and Field et al. (2012) (chapters 1 through 3 as well). As a last recommendation, you can also have a look at Winter (2020). 1.2 Get in touch If you have any questions about the information presented here or any other matters related to the class, please do not hesitate to drop me a line via email. Session Info ## R version 4.2.1 (2022-06-23) ## Platform: x86_64-apple-darwin17.0 (64-bit) ## Running under: macOS Big Sur ... 10.16 ## ## Locale: en_US.UTF-8 / en_US.UTF-8 / en_US.UTF-8 / C / en_US.UTF-8 / en_US.UTF-8 ## ## Package version: ## ggtext_0.1.1 forcats_0.5.2 stringr_1.4.1 dplyr_1.0.10 purrr_0.3.4 ## readr_2.1.2 tidyr_1.2.0 tibble_3.1.8 ggplot2_3.3.6 tidyverse_1.3.2 ## broom_1.0.1 patchwork_1.1.2 emmeans_1.8.0 afex_1.1-1 lme4_1.1-30 ## Matrix_1.4-1 here_1.0.1 nlme_3.1-159 fs_1.5.2 lubridate_1.8.0 ## httr_1.4.4 rprojroot_2.0.3 R.cache_0.16.0 numDeriv_2016.8-1.1 tools_4.2.1 ## backports_1.4.1 bslib_0.4.0 utf8_1.2.2 R6_2.5.1 DBI_1.1.3 ## colorspace_2.0-3 withr_2.5.0 tidyselect_1.1.2 compiler_4.2.1 cli_3.3.0 ## rvest_1.0.3 xml2_1.3.3 bookdown_0.28 sass_0.4.2 scales_1.2.1 ## mvtnorm_1.1-3 digest_0.6.29 minqa_1.2.4 R.utils_2.12.0 rmarkdown_2.16 ## pkgconfig_2.0.3 htmltools_0.5.3 styler_1.7.0 dbplyr_2.2.1 fastmap_1.1.0 ## rlang_1.0.5 readxl_1.4.1 rstudioapi_0.14 jquerylib_0.1.4 generics_0.1.3 ## jsonlite_1.8.0 R.oo_1.25.0 car_3.1-0 googlesheets4_1.0.1 magrittr_2.0.3 ## Rcpp_1.0.9 munsell_0.5.0 fansi_1.0.3 abind_1.4-5 R.methodsS3_1.8.2 ## lifecycle_1.0.1 stringi_1.7.8 yaml_2.3.5 carData_3.0-5 MASS_7.3-58.1 ## plyr_1.8.7 grid_4.2.1 crayon_1.5.1 lattice_0.20-45 haven_2.5.1 ## splines_4.2.1 gridtext_0.1.4 hms_1.1.2 knitr_1.40 pillar_1.8.1 ## [ reached getOption(&quot;max.print&quot;) -- omitted 23 entries ] References "],["2-concepts-and-background.html", "2 Concepts and Background 2.1 Some general points about the class 2.2 On inferential statistics 2.3 A note on frequentism 2.4 Hypotheses 2.5 Type I and Type II errors 2.6 Some terms 2.7 The \\(t\\)-test 2.8 Wrap-Up", " 2 Concepts and Background 2.1 Some general points about the class There might be some points we I explain something oddly or in a way that just does not click for you. In this case, please ask. Statistics is a little bit weird sometimes and there is no shame in not getting something the first time—As a case in point: I took my intro to stats twice. We will see how far we get each session. It is no problem at all if we do not cover all the material that is on the website for each slot. So, again, please ask any questions you may have. All the code is available on the website (with a handy button that pops up to copy it to your clipboard)—except for code for most of the plots. So please do not spend the time to copy the code by hand. All the data sets that I use are on github here: https://github.com/mkthalmann/intro-stats in the folder assets/data/. Please feel free to download them and play around. The code for the plots (and everyting else) can be found in the .Rmd files. The code for our practice sessions can be found on GitHub as well: assets/scripts/. If you want to make sure you have all the relevant packages you need for the class here, these are the main ones: pkg &lt;- c( &quot;here&quot;, # better working directory &quot;tidyverse&quot;, # data wrangling and plotting &quot;broom&quot;, # tidy lm summary &quot;afex&quot;, # ANOVA and linear mixed models &quot;lme4&quot; # linear mixed models ) install.packages(pkg) To load the packages, you can use the library() command with the (unquoted) package name. If you ever need a refresher on how a function works, say sum(), you can use the ? call: ?sum 2.2 On inferential statistics In essence, what we want to do is this: Find out which portions of variance in the sample are systematic (a pattern; introduced by us) and those which are unsystematic (coincidence; essentially random from the perspective of our experiment) and then try to relate this to the (unknown) population. Along the way, we will often see this intuition play out by dividing some measure of an effect (like the mean) by a measure of uncertainty (unsystematic variance, often the standard deviation in some form). To foreshadow a bit, both meansures of effect size – like Cohen’s \\(d\\) – and Pearson’s correlation coefficient \\(r\\) show this signal-to-noise ratio. And if you dive a bit deeper, it pops up all over the place – one such place is the \\(t\\) statistic we use to get a \\(p\\) value for the \\(t\\)-test, see below: This is, our intuitive formula, \\[\\mathit{signal-to-noise ratio} = \\frac{\\mathit{systematic\\ variance}}{\\mathit{unsystematic\\ variance}}\\], looks very similar to the one for the \\(t\\) statistic: \\[t = \\frac{\\bar{x}_1-\\bar{x}_2}{\\mathit{SE}}\\] So we take a sample and measure some quantity and then we draw an inference towards the population. Our ingredients for this are always the same: effect size (namely a difference in measurements), variability, and sample size. Note that the population is not actually easily defined in linguistics. Certainly, we want to talk about speakers of a language or dialect as a (large but finite) population. But there is another answer to the population question, and that has to do with the linguistic material. And as generative system, the population we want to generalize towards is often infinite. In fact, despite the centrality of this question and its potential answers, we will postpone this discussion until the very end. The point where we say that speakers and potential sentences (whatevery exactly that means) are a part of our relevant populations will be the point where we will discuss (linear) mixed models, the new workhorse in linguistic experiments and corpus analyses. Systematic vs unsystematic variance, from Sammut and Webb (2010, 101) 2.3 A note on frequentism Frequentism, the (class of) approach(es) we will follow in this course, interprets probabilities as relative frequencies over lots of samples from a population. So, to use old example, a single coin toss does not have a meaningful probability. Rather, in the long run (as the number of samples approach infinity), we expect a fair coin to approach a probability of \\(.5\\) for either result. This intuition will pop up all over the place, most prominently with confidence intervals. 2.3.1 Measures of Central Tendency Often, we will work with means, a measure of central tendency. \\[\\bar{x}={\\frac {1}{n}}\\sum _{i=1}^{n}a_{i}={\\frac {a_{1}+a_{2}+\\cdots +a_{n}}{n}}\\] mean(1:10) ## [1] 5.5 Figure 2.1: Means as a location parameter. Measures of central tendency are the most vital indexes of experimental results. They indicate where the mass of your measured property is located. Though you will work with means most often, we will touch on some others as well. The median always leads to a bipartition of the data. The partition on a set \\(A\\) is defined as having the following properties : for any two distinct subsets, candidates for cells of a partition of \\(A\\), \\(X\\) and \\(Y\\): \\(X \\cap Y = \\emptyset\\), and the union of all partitions must equal \\(A\\): for two partitions \\(X\\) and \\(Y\\), \\(X \\cup Y = A\\) To calculate the mean, we just do the following: \\[\\tilde{x}=\\begin{cases} x_{\\frac{n+1}{2}} &amp; n\\text{ odd} \\\\ \\frac{1}{2}\\left(x_{\\frac{n}{2}} + x_{\\frac{n}{2} + 1}\\right) &amp; n \\text{ even} \\\\ \\end{cases}\\] median(1:10) ## [1] 5.5 This partition property is illustrated below for a normally-distributed data set: Figure 2.2: Medians as partitions. The mode is not often used today, though it does come up when people talk about bimodal patterns or distributions. If you find this kind of result within one level of your experimental manipulation, proceed with caution, as the mean is no longer a good fit for data. mode &lt;- function(x) { ux &lt;- unique(x) tab &lt;- tabulate(match(x, ux)) ux[tab == max(tab)] } mode(c(1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10)) ## [1] 7 Figure 2.3: Modes. 2.3.2 Measures of Dispersion Measures of dispersion, different from the location measures we saw before, index the spread of a finding, where a low value always indicates more precision/homogeneity. Most often, these are used to indicate uncertainty and, hence, non-systematic variance. Though we do want to minimize the spread of a data, too little spread is not a good thing either: Without any variance at all, most statistical techniques fail outright or become unreliable. Too little variance can also indicate that something went wrong. If a fellow linguist told you that, in their behavioral experiments, all people from one group produced exactly (or close to) the same results, chances are you wouldn’t believe them. Sums of squares Sums of squares are a way to indicate the general deviation of a set of samples from the (group) mean. Though this is not often mentioned in papers, this technique underlies most classical tests in statistics. When people talk about OLS (Ordinary Least Squares) regression, they are describing a technique where a line is fit to the data that minimizes squared differences from the mean. Tests of this type are ANOVA, the \\(t\\)-test, and Linear Mixed Models. More succinctly, the sum of squared differences is calculated as follows: \\[\\mathit{SumSq} = \\sum _{i=1}^{n}(x_i-\\bar{x})^2\\] sumsq &lt;- function(x) { sum((x - mean(x))^2) } sumsq(1:10) ## [1] 82.5 Figure 2.4: Mean deviations. The solution: Squaring the differences to remove the sign (and by transforming distances from the mean into planes): Figure 2.5: Sums of squares. Variance and standard deviation The standard deviation is based on the variance of a sample, and in turn, the sum squared differences from the mean. The variance is calculated like so: \\[s^2= {\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar{x}})^{2}}{n-1}}\\] var(1:10) ## [1] 9.1666667 Because of the squaring of the differences, however, the resulting value for variance in not on the measure scale anymore. In fact, it is on a squared version of that. To remedy this, and to get to the standard deviation, we simply take the root: \\[s= \\sqrt{s^2} = {\\sqrt{\\frac {\\sum _{i=1}^{n}(x_{i}-{\\bar{x}})^{2}}{n-1}}}\\] sd(1:10) ## [1] 3.0276504 sqrt(var(1:10)) ## [1] 3.0276504 Standard Deviation The larger the dispersion, the flatter our resulting curve: Dispersion changes affect curvature Standard Errors and Confidence Intervals The standard error of the mean is a measure of the dispersion of sample means around the population mean, and decreases as a function of the root of \\(n\\): \\[s_{\\bar{x}} = \\frac{s}{\\sqrt{n}}\\] se &lt;- function(x) { sd(x) / sqrt(length(x)) } se(1:10) ## [1] 0.95742711 Note that this measure is powerful because it takes the sample size into account. While we saw with, say, the standard deviation features, this is done to create sort of a mean of variance. Here, the use is different: We will obtain smaller standard errors (which will make us more sure about the precision of our measurement) as a function of sample size. Measuring the same variance of two samples, we could use the standard error to relate the two in terms of precision such that the larger sample makes us more confident. Confidence intervals for the mean give us a range of values around the mean where we expect the ``true’’ (population) mean is located. To quantify the level of certainty we’re comfortable with, often 95% is chosen (in analogy to \\(p\\)-values). The sample mean plus or minus \\(1.96\\) times its standard error gives us the 95% confidence interval: \\[CI_{95} = \\bar{x} \\pm SE_{\\bar{x}}*1.96\\] But how do you interpret confidence intervals? Like so: 95% of 95% confidence intervals contain the true (population) parameter you are trying to approximate using your experiment (most generally the mean). In a parallel fashion, or 90% confidence intervals, out of a 100, 90 will contain the true parameter. This also means that 95% confidence intervals will always be wider than 90% ones if they are based on the same data: If you can only be wrong 5% of the time, you will be less sure than if you can be wrong in a tenth of all cases. 2.4 Hypotheses 2.4.1 Null Hypothesis People are often confused as to why they should think about a null hypothesis in the first place. After all, the only thing we care about is our actual (in statistics parlance alternative) hypothesis. One clue to answering this question is below: The key idea for inferential statistics is as follows: If we know what a “random” distribution looks like, we can tell random variation from non-random variation. We will start by supposing that the variation observed is random – and then try to prove ourselves wrong. This is called ``Making the null hypothesis.’’ Vasishth and Broe (2010) Another clue is this: \\(p\\)-values do not say anything about the alternative hypothesis at all. They only inform us about how likely our data are given the assumption that the null hypothesis holds. Naturally, this is rarely the question we want answered, but it is the only answer that can be answered meaningfully using frequentist statistics. Figure 2.6: Null hypothesis. The null hypothesis thus says that all differences we might observe are non-systematic, or random, in nature such that experimental manipulations do not produce predictable results. This is the assumption we are trying to falsify using significance tests. In other words, we want to show that there does, in fact, exist a systematic, non-random relationship between our factors (our manipulation) and the dependent variable (our measurement) such that all variance is systematic (at least overwhelmingly so). Questions What is the inherent problem of this approach, namely the primacy of the null hypothesis within frequentist statistics — and, in turn, the secondary role that the alternative hypothesis plays? 2.4.2 Alternative hypothesis We predict patterns in the population, and using inference statistics we try to find evidence for this pattern using samples of that population. And even if our statistics say that the effect we found is systematic, it still might not be so (spurious results) Figure 2.7: Alternative hypothesis. Here is an illustration of the two competing hypotheses using distributions, identified by two sample means \\(x_1\\) and \\(x_2\\): Figure 2.8: Alternative hypothesis again. And here we have a comparison between the null and the alternative hypothesis: Figure 2.9: left: Null hypothesis, right: Alternative hypothesis.. 2.5 Type I and Type II errors So, our goal is to discriminate between the null and the alternative hypothesis. Depending on our \\(p\\)-value, we will either accept the null hypothesis (\\(p &gt; .05\\)) or reject it (\\(p &lt; .05\\)). But since we’re always aware that we might be wrong—after all, our significance threshold has this possibility built-in by design—there a four possible outcome scenarios: The null hypothesis is … true false rejected (\\(p &lt; .05\\)) Type I error (false positive) Correct decision (true positive) accepted (\\(p &gt; .05\\)) Correct decision (True negative) Type II error (false negative) A closely related concept to statistical errors is power, which is the probability of detecting an effect that is actually there. Power is determined by three things: Significance (higher \\(\\alpha\\) = more power) Sample size (larger \\(N\\) = more power) Effect size (larger effect = more power) In principle, when designing an experiment, the only one of these factors you have any control over is the sample size. So be wary of experiments with few items and participants, both when designing your own and when reading a paper. In the literature you will see two kinds of power analysis mentioned, a priori, i.e., calculated before the study was carried out, and post hoc, carried out afterwards. The second way is seriously flawed and is dependent upon the \\(p\\)-value. So it is not useful at all. Do not do this. 2.6 Some terms When describing (and planning) an experiment, this is ideally done using the following terms: Independent Variable: What you manipulate. Dependent Variable: What you measure. Main effects are the differences that, for example, a \\(t\\)-test is sensitive to: namely a difference between condition means. Here is a classical situation for a one-factorial design: Figure 2.10: One factorial design (two factor levels). 2.7 The \\(t\\)-test Now, let’s actually get to the statistics and apply what we’ve learned so far. Below, I have a data set containing (among other information), lung capacities and the smoker status of of a number of people—downloaded here. Say our (alternative) hypothesis is that smokers and non-smokers have a different lung capacity, while the null hypothesis is that there is no difference between the two. As always, we have no way of testing the entire population, so we will have to make do with our sample data set and draw an inference towards the state that is likely to hold in the population. This is where the \\(t\\)-test comes in. (I am neglecting here the one-sample \\(t\\)-test, assuming that you are already familiar, and jumping straight into the two-sample version, which is more useful for our purposes anyhow.) lungs &lt;- read.csv( here(&quot;assets&quot;, &quot;data&quot;, &quot;lungs.txt&quot;), sep = &quot;|&quot;, dec = &quot;,&quot; ) %&gt;% rename( &quot;lung_cap&quot; = Lungenkapazität, &quot;age&quot; = Alter, &quot;height&quot; = Größe, &quot;smoker&quot; = Raucher, &quot;sex&quot; = Geschlecht, &quot;caesarean&quot; = Kaiserschnitt ) We could use means, standard deviations, and the standard errors of the mean to get a sense of the data: lungs %&gt;% group_by(smoker) %&gt;% summarise( lung_cap_mean = mean(lung_cap), lung_cap_sd = sd(lung_cap), lung_cap_se = lung_cap_sd / sqrt(n()), how_many = n() ) ## # A tibble: 2 × 5 ## smoker lung_cap_mean lung_cap_sd lung_cap_se how_many ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 no 7.73 2.73 0.121 505 ## 2 yes 8.75 1.86 0.248 56 But it’s also always good to have a plot: Figure 2.11: Smokers Remember, our hypothesis is that there is a difference in lung capacity with smokers and non-smokers. Thus, our dependent variable is the lung capacity of each person in the data set, the independent variable is the smoker status. And what we predict is a main effect of smokerdom. Now, say we have an argumentative friend who says the data in the plot is neither here nor there. He wants a definitive answer. While statistics is certainly not definitive, it is certainly better than nothing. So we do a \\(t\\)-test to compare the lung capacity between the two groups. Under the hood (more on that below), the mean difference is calculated (or indicator of systematic variance) and divided by (a version of) the standard errors of each sample, which represent the variation we have no explanation for. A larger \\(t\\) value is then an indicator for a strong effect of the experimental manupulation, a small one suggests that most of the variation is not due to the factor of interest. t.test( lung_cap ~ smoker, data = lungs, paired = FALSE, var.equal = TRUE, alternative = &quot;two.sided&quot; ) ## ## Two Sample t-test ## ## data: lung_cap by smoker ## t = -2.71912, df = 559, p-value = 0.006749 ## alternative hypothesis: true difference in means between group no and group yes is not equal to 0 ## 95 percent confidence interval: ## -1.75127037 -0.28228515 ## sample estimates: ## mean in group no mean in group yes ## 7.7309901 8.7477679 Question: So, what does this mean? As an aside: We do not care about the sign of the \\(t\\)-value. This is only a reflex of the ordering of the grouping factor in R. If we change that, the negative sign goes away, but everything else remains the same: lungs %&gt;% mutate(smoker = fct_relevel(smoker, &quot;yes&quot;)) %&gt;% t.test( lung_cap ~ smoker, data = ., paired = FALSE, var.equal = TRUE, alternative = &quot;two.sided&quot; ) ## ## Two Sample t-test ## ## data: lung_cap by smoker ## t = 2.71912, df = 559, p-value = 0.006749 ## alternative hypothesis: true difference in means between group yes and group no is not equal to 0 ## 95 percent confidence interval: ## 0.28228515 1.75127037 ## sample estimates: ## mean in group yes mean in group no ## 8.7477679 7.7309901 We could report this (to our annoying friend) by saying the following: The difference in lung capacity between smokers and non-smokers was significant, \\(t(559) = -2.7, p &lt; .05\\). This means that, under the assumption that the null hypothesis is true in the population, finding the test statistic that we found in our sample (or a more extreme one) is incredibly unlikely, namely \\(p = 0.0067\\). To reiterate: the \\(p\\)-value does not tell us anything about the alternative hypothesis. It only tells us that our data are very surprising if we expect the true difference to be \\(0\\). We often do some mental gymnastics and use imprecise/wrong formulations, but it is important to remember that a \\(p\\)-value cannot give you a full solution to your problem, it is only tool in your toolbox to argue for something (e.g. that the alternative hypothesis is true). Normally we say that a result is statistically significant when the \\(p\\)-value is low enough, namely below the so-called \\(\\alpha\\) criterion, generally at 5%. Anything above this threshold is not surprising enough, and we conventionally say that that is not enough to establish statistical significance. But this is an arbitrary number—there is nothing inherent to this setting of \\(\\alpha\\) that makes it worthy as the make-it-or-break-it line over any other. Here’s the \\(t\\)-test formula given on Wikipedia (for equal or unequal sample sizes, similar variances): \\[t = \\frac{\\bar{X}_1 - \\bar{X}_2}{s_p * \\sqrt{\\frac{1}{n_1}+\\frac{1}{n_2}}}\\] where \\(s_p\\) represents the pooled variances (and thus the random variation we have no explanation for): \\[s_p = \\sqrt{\\frac{(n_1 - 1) s^2_{X_1} + (n_2 - 1) s^2_{X_2} }{n_1 + n_2 - 2}}\\] Note that the numerator represents the systematic variation, i.e., the difference between the two means. By dividing the systematic variance by the unsystematic one, we get an indication of how systematic the mean differences actually are. And now let’s implement this in R and see whether we can get the same results that we got above using t.test(). Task: If you guys want to, you could do this on your own. Of course, you have my solution there but if you truly want to understand what’s going on, it might be good x1 &lt;- lungs %&gt;% filter(smoker == &quot;no&quot;) %&gt;% pull(lung_cap) x2 &lt;- lungs %&gt;% filter(smoker == &quot;yes&quot;) %&gt;% pull(lung_cap) # means x_bar_1 &lt;- mean(x1) x_bar_2 &lt;- mean(x2) # n_i n_1 &lt;- length(x1) n_2 &lt;- length(x2) # variances s_1_squared &lt;- var(x1) s_2_squared &lt;- var(x2) # pooled estimator: s_p &lt;- ((n_1 - 1) * s_1_squared + (n_2 - 1) * s_2_squared) / (n_1 + n_2 - 2) s_p &lt;- sqrt(s_p) # calculate the t value t &lt;- (x_bar_1 - x_bar_2) / (s_p * sqrt(1 / n_1 + 1 / n_2)) t ## [1] -2.7191178 # degrees of freedom dof &lt;- n_1 + n_2 - 2 # look up the p value in the t distribution using the degrees of freedom (two-tailed) p &lt;- 2 * pt(t, dof, lower.tail = TRUE) p ## [1] 0.0067490382 And here is another way of getting the same results, the lm() function: t.test(lung_cap ~ smoker, data = lungs, var.equal = TRUE) ## ## Two Sample t-test ## ## data: lung_cap by smoker ## t = -2.71912, df = 559, p-value = 0.006749 ## alternative hypothesis: true difference in means between group no and group yes is not equal to 0 ## 95 percent confidence interval: ## -1.75127037 -0.28228515 ## sample estimates: ## mean in group no mean in group yes ## 7.7309901 8.7477679 summary(lm(lung_cap ~ smoker, data = lungs)) ## ## Call: ## lm(formula = lung_cap ~ smoker, data = lungs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.70599 -1.68099 0.09401 1.94401 6.94401 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.73099 0.11814 65.4372 &lt; 0.00000000000000022 *** ## smokeryes 1.01678 0.37394 2.7191 0.006749 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.6549 on 559 degrees of freedom ## Multiple R-squared: 0.013054, Adjusted R-squared: 0.011288 ## F-statistic: 7.3936 on 1 and 559 DF, p-value: 0.006749 2.8 Wrap-Up That’s it for today. Here’s a small recap of what we looked at: We talked about the primary method of inferential statistics: separating out systematic variance (the stuff that we care about) from (in the context of our experimental goal) randomness. Then we saw some of the estimators that play a central role in discriminating between the two. To do this, we often use the mean and variance/standard deviation. After that we talked about hypotheses and saw that frequentist statistics really only talks about the null hypothesis, even though we only care about the alternative hypothesis. remember: the alternative hypothesis should be written down and ideally graphed before data collection begins to keep yourself accountable. Finally we worked through an example data set with the \\(t\\)-test, both with the t.test() function and by hand (the latter again involved the measures of central tendency and of dispersion), and tried to interpret the \\(p\\)-value both returned. In tomorrow’s class we will continue on our stats journey and talk about more complex designs. References "],["3-anova.html", "3 ANOVA 3.1 Yesterday 3.2 How to get to Rome 3.3 Repeated Measures 3.4 Two-Way ANOVA 3.5 More repeated measures 3.6 ANOVA by hand 3.7 Interpreting an ANOVA 3.8 Wrap-Up", " 3 ANOVA 3.1 Yesterday So, yesterday we talked about various things: measures of central tendency and of dispersion null hypotheses and alternative hypotheses a short review of the \\(t\\) test Today’s plan is to introduce cases where we have more than independent variable—situations where we have more than one main effect. In these cases, our \\(t\\)-test does not work: t.test(lung_cap ~ smoker + age, data = lungs) Error in t.test.formula(lung_cap ~ smoker + age, data = lungs): ‘formula’ missing or incorrect Question: Can you think of a reason why we cannot just several \\(t\\)-tests here? There are at least two: one has to do with the kind of data we have here and one has to do with a concept from yesterday: power. This means that we need to talk about more sophisticated tests that let us draw an inference on the basis of multiple factors. We will also talked about a very central topic in the analysis of (psycho)linguistic data: repeated measures. In contrast to the lung capacity data we saw yesterday where every person had the lung capacity measured once, in linguistics we will often give participants multiple items of the same condition. This means that we have several data points per person per experimental condition. And one of our goal for today will be to integrate this design feature into our statistical toolbox. 3.2 How to get to Rome Before we do so, let me briefly say something about equivalence. Yesterday, we used a very specific function, namely t.test(), to derive our statistical result. However, especially at this simple level where we’re only dealing with a single independent variable, a lot of tests will give us the same result. Before we move on to the illustration, let me introduce a new data set. This is from an experiment I did with two colleagues of mine: Yuqui Chen and Mailin Antomo (Chen et al. 2022). Here are two sample items. People saw all items (several hard and soft triggers), but only one condition per item, either the assertion was at-issue or the presupposition. Soft trigger win The panda, the duck, and the frog are having a drawing competition. All like the duck’s drawing the most. The duck receives a crown as a prize. The clown did not pay attention again and asks: Assertion at-issue: “Did the duck do the best at the competition?” Presupposition at-issue: “Did the duck participate in the competition?” Little Peter responds, “The duck won the competition.” Hard trigger again The panda is a member of a soccer club. Recently, he has been playing very well and scored many goals in the past weeks. Yesterday there was a football game and he scored a goal. The clown did not pay attention again and asks: Assertion at-issue: “Did the panda score a goal yesterday?” Presupposition at-issue: “Did the panda score a goal for the first time yesterday?” Little Peter responds, “The panda scored a goal again yesterday.” A sample of our image material for the “win” item. The (unfortunately very ugly) Likert-type scale from 1 (unacceptable) to 5 (acceptable) We will only work with a smaller set of data here. As you can see in the code below, I am excluding the children, and one level of the trigger type independent variable (namely non-restrictive relative clauses). And there is some other stuff in the columns that I am dropping here, but this level of complexity should be enough for now. d_psp &lt;- read_csv(here(&quot;assets&quot;, &quot;data&quot;, &quot;psp-data.csv&quot;)) %&gt;% filter(trigger_cat != &quot;appo&quot;, stage != &quot;Children&quot;) %&gt;% select(id, itemid, trigger_cat, issue, judgment) d_psp ## # A tibble: 657 × 5 ## id itemid trigger_cat issue judgment ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 auncqkpvl6b3rp0d3g833vmuh6 gewinnen2 soft at-issue 5 ## 2 auncqkpvl6b3rp0d3g833vmuh6 schaffen1 soft at-issue 1 ## 3 auncqkpvl6b3rp0d3g833vmuh6 auch3 hard at-issue 1 ## 4 auncqkpvl6b3rp0d3g833vmuh6 entdecken1 soft at-issue 5 ## 5 auncqkpvl6b3rp0d3g833vmuh6 schaffen3 soft at-issue 1 ## 6 auncqkpvl6b3rp0d3g833vmuh6 cleft3 hard non-at-issue 5 ## 7 auncqkpvl6b3rp0d3g833vmuh6 wieder1 hard non-at-issue 5 ## 8 auncqkpvl6b3rp0d3g833vmuh6 auch2 hard non-at-issue 5 ## 9 auncqkpvl6b3rp0d3g833vmuh6 auch1 hard at-issue 1 ## 10 auncqkpvl6b3rp0d3g833vmuh6 entdecken2 soft non-at-issue 5 ## # … with 647 more rows Here’s a plot of the data: Now, here’s the lm() call that gives us the difference betweem the two trigger types (remember that this is equivalent to the \\(t\\) test): summary(lm(judgment ~ trigger_cat, data = d_psp)) ## ## Call: ## lm(formula = judgment ~ trigger_cat, data = d_psp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.045455 -1.244648 -0.045455 0.954545 1.755352 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.244648 0.076661 42.3246 &lt; 0.00000000000000022 *** ## trigger_catsoft 0.800806 0.108168 7.4033 0.0000000000004096 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.3863 on 655 degrees of freedom ## Multiple R-squared: 0.077217, Adjusted R-squared: 0.075808 ## F-statistic: 54.809 on 1 and 655 DF, p-value: 0.00000000000040959 And here’s one more equivalent command (though you do not see the \\(t\\) value below, if you look at the results above, you will see the \\(F\\) value at the very bottom and that the \\(p\\)-values are the same): summary(aov(judgment ~ trigger_cat, data = d_psp)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trigger_cat 1 105.33 105.330 54.809 0.0000000000004096 *** ## Residuals 655 1258.75 1.922 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 So, in this simple case, the following three commands produce identical results: t.test(), lm(), aov(). But what is this aov() command exactly? It stands for the test people commonly call ANOVA (ANalysis Of VAriance). For a long while it was the standard test in linguistics (and related disciplines). For us, it is important to understand it as an extension of the \\(t\\)-test: The two models are exactly identical in the simple case with one predictor—for the ANOVA, this is often called one-way ANOVA. In fact, the same computation we already did for the \\(t\\)-test will show up again with the ANOVA, only the names for the parts will be different. Importantly, the ANOVA, but not the \\(t\\)-test, handles more complex analytical situations as well, namely ones where we want to include several independent variables. And of course, in these situations the equivalence breaks down. What about the lm()? LM is short for linear model, i.e., a type of analysis that works by fitting a line by minimizing the distance of the actual observations to the line. Again, this line fitting procedure is equivalent to the ANOVA and the \\(t\\)-test in this case. set.seed(1234) sim &lt;- tibble( a = rnorm(n = 15, mean = 2), b = rnorm(n = 15, mean = 3), ) %&gt;% pivot_longer(cols = c(a, b), names_to = &quot;cond&quot;) sim ## # A tibble: 30 × 2 ## cond value ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 0.793 ## 2 b 2.89 ## 3 a 2.28 ## 4 b 2.49 ## 5 a 3.08 ## 6 b 2.09 ## 7 a -0.346 ## 8 b 2.16 ## 9 a 2.43 ## 10 b 5.42 ## # … with 20 more rows t.test(value ~ cond, data = sim, var.eqal = TRUE) ## ## Welch Two Sample t-test ## ## data: value by cond ## t = -3.22713, df = 27.9739, p-value = 0.0031812 ## alternative hypothesis: true difference in means between group a and group b is not equal to 0 ## 95 percent confidence interval: ## -1.76840555 -0.39508234 ## sample estimates: ## mean in group a mean in group b ## 1.662703 2.744447 summary(lm(value ~ cond, data = sim)) ## ## Call: ## lm(formula = value ~ cond, data = sim) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.00840 -0.57440 -0.21824 0.56148 2.67139 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.66270 0.23702 7.0149 0.0000001251 *** ## condb 1.08174 0.33520 3.2271 0.003179 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.91799 on 28 degrees of freedom ## Multiple R-squared: 0.27111, Adjusted R-squared: 0.24507 ## F-statistic: 10.414 on 1 and 28 DF, p-value: 0.0031789 summary(aov(value ~ cond, data = sim)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cond 1 8.7763 8.7763 10.414 0.003179 ** ## Residuals 28 23.5959 0.8427 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Notice here that \\(t^2 = F: 3.2271^2 = 10.414\\). And notice that the coefficient for the slope of cond is the same as the mean difference we find for the \\(t\\)-test: t.test(value ~ cond, data = sim) %&gt;% tidy() %&gt;% select(1:3) ## # A tibble: 1 × 3 ## estimate estimate1 estimate2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -1.08 1.66 2.74 What is important to remember is this: All of the tests we have discussed so far share a common core, and are actually mathematically identical in the simple case. With this in mind, let’s abandon the simple case and move on to repeated measures. 3.3 Repeated Measures As I hinted at at the outset, we ignored a very systematic part of the variance in the model above: repeated measures. This is the situation where we have multiple data points from the same person (each person judged different items in the same condition). While this type of systematic variance is not necessarily due to our experimental manipulations (more on that a little later when we talk about random slopes), it would be bad to ignore it because it may be a confound in our data. That is, a source of systematic variation in our data that has nothing to do with our experiment. To see this, let’s look at some of the participants in our experiment: As we can see, though all participants show the overall effect, they differ from each other in quite significant ways. This individual variation is something we would like to account for in our statistics. For the aov function call, it is relatively easy to add repeated measures, simply add the grouping variable (the people) and the factors whose different levels were tested with each of them (trigger type) as an Error() term in your model. anova_psp &lt;- aov(judgment ~ trigger_cat + Error(id / (trigger_cat)), data = d_psp) summary(anova_psp) ## ## Error: id ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trigger_cat 1 1.041 1.0408 0.2506 0.6202 ## Residuals 31 128.752 4.1533 ## ## Error: id:trigger_cat ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## trigger_cat 1 104.954 104.954 48.21 0.00000007297 *** ## Residuals 32 69.665 2.177 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Error: Within ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Residuals 591 1059.7 1.793 We could report this like this: The predicted main effect of TRIGGER TYPE was significant, \\(F(1,31) = 48.21, p &lt; .001\\). A lot of people find the following ANOVA command from the afex package (Singmann et al. 2016) a little bit easier to use, so I am including it here. library(afex) afex_psp &lt;- aov_ez( &quot;id&quot;, &quot;judgment&quot;, within = &quot;trigger_cat&quot;, data = d_psp, print.formula = TRUE ) afex_psp ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 trigger_cat 1, 32 0.22 48.54 *** .347 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 So, what we’ve done so far: We moved from the 3.4 Two-Way ANOVA Let’s look at the plot for the presupposition data again: From the plot (and the original experiment) is set up, you can see that there seems to be another factor at play: at-issueness. In the critical items, the clowns question either targeted the foregrounded (at-issue) portion of the critical utterance or the backgrounded, presupposed material. As you can see, this factor seems to be quite important because it drives a quite severe judgment asymmetry for the hard triggers: When the presupposition of a hard trigger is used to answer a question, infelicity ensues. For the soft triggers, this is much less pronounced. In what is to come, we will incorporate two factors into our model. As we saw before, the \\(t\\)-test is no longer applicable in these cases, so we will work with the ANOVA for now. psp_two &lt;- aov_car(judgment ~ trigger_cat + issue + Error(id / (trigger_cat + issue)), data = d_psp) ## Warning: More than one observation per design cell, aggregating data using `fun_aggregate = mean`. ## To turn off this warning, pass `fun_aggregate = mean` explicitly. psp_two ## Anova Table (Type 3 tests) ## ## Response: judgment ## Effect df MSE F ges p.value ## 1 trigger_cat 1, 32 0.43 47.05 *** .263 &lt;.001 ## 2 issue 1, 32 0.29 286.14 *** .589 &lt;.001 ## 3 trigger_cat:issue 1, 32 0.25 117.52 *** .337 &lt;.001 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;+&#39; 0.1 &#39; &#39; 1 The first two lines in the table are our main effects, which say that the trigger type and at-issueness manipulations lead to significantly different judgments. Question: But what about the third line? The third line is a so-called interaction. Recall that main effects describe significant differences. Interactions are probably best conceptualized as differences between differences. As we saw in the plot, the at-issueness factor seems to affect hard presupposition triggers more strongly than soft ones. In a sense, we need to look at the settings of both manipulations in order to predict the judgments that participants are likely to make. Note that this also affects our main effect for issueness. Before we simply that there is a significant difference between the two levels of this factor. However, with both the plot and the significant interaction under our belt, we now that we should be a little more careful: While there is something going on with at-issueness, the main effect is mainly driven by the hard presupposition triggers, with the soft triggers, the effect is almost negligible. In the plots below, we have some more interactions: Above: main effect possibilities without interactions below: including interactions – main effects determined by means, interactions by slopes. Ltr/ttb: no main effect, main effect for \\(x\\) factor, split factor main effect, main effects for both, no main effect but interaction, \\(x\\) factor effect plus interaction, split factor effect plus interaction, two main effects plus interaction. Figure 3.1: Interactions 3.5 More repeated measures Question: Is there another repeated measures source in our data? Have a look again and see if you can spot one. ## # A tibble: 657 × 5 ## id itemid trigger_cat issue judgment ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 auncqkpvl6b3rp0d3g833vmuh6 gewinnen2 soft at-issue 5 ## 2 auncqkpvl6b3rp0d3g833vmuh6 schaffen1 soft at-issue 1 ## 3 auncqkpvl6b3rp0d3g833vmuh6 auch3 hard at-issue 1 ## 4 auncqkpvl6b3rp0d3g833vmuh6 entdecken1 soft at-issue 5 ## 5 auncqkpvl6b3rp0d3g833vmuh6 schaffen3 soft at-issue 1 ## 6 auncqkpvl6b3rp0d3g833vmuh6 cleft3 hard non-at-issue 5 ## 7 auncqkpvl6b3rp0d3g833vmuh6 wieder1 hard non-at-issue 5 ## 8 auncqkpvl6b3rp0d3g833vmuh6 auch2 hard non-at-issue 5 ## 9 auncqkpvl6b3rp0d3g833vmuh6 auch1 hard at-issue 1 ## 10 auncqkpvl6b3rp0d3g833vmuh6 entdecken2 soft non-at-issue 5 ## # … with 647 more rows Yes, we also have several items for each condition. But how do we deal with this in the context of our ANOVA? We cannot, unfortunately, have both a by-subject ANOVA and a by-items ANOVA at the same time. So, what people do in these cases is to compute two ANOVAs and report both – often, the by-participants ANOVA is reported as \\(F_1\\), and the by-items ANOVA as \\(F_2\\). This then means that for each effect, you would need both of the ANOVAs to yield a significant result. 3.6 ANOVA by hand In my own first encounter with statistics, I had to do an ANOVA by hand. While at the time I thought it was a little grueling, in the end I thought it was one of the most important excercises. So, I apologize in advance. This is the data we will be working with: d &lt;- tibble( student = c(&quot;Raphael&quot;, &quot;Nicolaus&quot;, &quot;Balthasar&quot;, &quot;Claudia&quot;, &quot;Heike&quot;, &quot;Marion&quot;), time1 = c(14.3, 16.2, 17.2, 12.8, 14.9, 11.1), time2 = c(19.2, 15.9, 17.2, 20.3, 25.3, 12.1), time3 = c(27.3, 33.5, 42.9, 33.2, 58.3, 39.1) ) d ## # A tibble: 6 × 4 ## student time1 time2 time3 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raphael 14.3 19.2 27.3 ## 2 Nicolaus 16.2 15.9 33.5 ## 3 Balthasar 17.2 17.2 42.9 ## 4 Claudia 12.8 20.3 33.2 ## 5 Heike 14.9 25.3 58.3 ## 6 Marion 11.1 12.1 39.1 As you might know, it often desirable to have the data in what is called the long format (as opposed to the wide one we have up there). Here is a way to achieve that: d &lt;- d %&gt;% pivot_longer( cols = c(time1, time2, time3), names_to = &quot;when&quot;, values_to = &quot;time&quot; ) d ## # A tibble: 18 × 3 ## student when time ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 Raphael time1 14.3 ## 2 Raphael time2 19.2 ## 3 Raphael time3 27.3 ## 4 Nicolaus time1 16.2 ## 5 Nicolaus time2 15.9 ## 6 Nicolaus time3 33.5 ## 7 Balthasar time1 17.2 ## 8 Balthasar time2 17.2 ## 9 Balthasar time3 42.9 ## 10 Claudia time1 12.8 ## 11 Claudia time2 20.3 ## 12 Claudia time3 33.2 ## 13 Heike time1 14.9 ## 14 Heike time2 25.3 ## 15 Heike time3 58.3 ## 16 Marion time1 11.1 ## 17 Marion time2 12.1 ## 18 Marion time3 39.1 With the way the data is structured above, we see the repeated measures much more clearly: For each participant, we have several rows, each of which contain one observation. Before we jump into things, here’s a plot: Figure 3.2: top: overall times, bottom: individual times. In general, the ANOVA is just like the \\(t\\)-test in that we divide the systematic variance by the unsystematic variance. You already know that the statistic that underlies the ANOVA is the \\(F\\) value: \\[F = \\frac{MS_{model}}{MS_{error}}\\] And the general intuition is this: We can divide the variance in a data set into these two portions in the equation above: \\[s^2_{total} = s^2_{model} + s^2_{error}\\] As the first step, we want the total sum of squares, where \\(i\\) is the index of the factor levels and \\(y_{im}\\) is the single observation of one participant under a certain factor level: \\[QS_{total} = \\sum\\limits_{i}\\sum\\limits_{m} (y_{im} - \\bar{x})^2\\] Now, we want the variance that the factors contribute to the total variance. \\(i\\) is again the index for the factor levels and \\(n\\) is the number of observations for that factor level: \\[QS_{model} = \\sum\\limits_{i} n \\cdot (\\bar{x}_i - \\bar{x})^2\\] Next step: We need the source of random variation, which is represented by the difference of the single observations from the group mean (\\(y_{mi}\\) is the value of the participant \\(m\\) measured at factor level \\(i\\): \\[QS_{error} = \\sum\\limits_{i}\\sum\\limits_{m} (y_{im} - \\bar{x}_i)^2\\] As with the \\(t\\)-test, we also need the degrees of freedom, where \\(N\\) is the total number of observations, \\(p\\) is the number of factor levels: \\[df_{QS_{total}} = N - 1\\] \\[df_{QS_{model}} = p - 1\\] \\[df_{QS_{error}} = N - p\\] For the final step, we need to account for the fact the we used more observations for the \\(QS_{error}\\) (18) than we did for the \\(QS_{model}\\) (3), so we need to normalize the sums of squares by dividing them by the corresponding degrees of freedom, which gives us the mean squares: \\[MS_{error} = \\frac{QS_{error}}{df_{error}}\\] \\[MS_{model} = \\frac{QS_{model}}{df_{model}}\\] And then we have everything to do the final computation: \\[F = \\frac{MS_{model}}{MS_{error}}\\] Now, we need the grand mean and the condition means for each time point: d &lt;- d %&gt;% mutate( grand_mean = mean(time), mean_run1 = mean(d$time[d$when == &quot;time1&quot;]), mean_run2 = mean(d$time[d$when == &quot;time2&quot;]), mean_run3 = mean(d$time[d$when == &quot;time3&quot;]) ) d ## # A tibble: 18 × 7 ## student when time grand_mean mean_run1 mean_run2 mean_run3 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raphael time1 14.3 23.9 14.4 18.3 39.0 ## 2 Raphael time2 19.2 23.9 14.4 18.3 39.0 ## 3 Raphael time3 27.3 23.9 14.4 18.3 39.0 ## 4 Nicolaus time1 16.2 23.9 14.4 18.3 39.0 ## 5 Nicolaus time2 15.9 23.9 14.4 18.3 39.0 ## 6 Nicolaus time3 33.5 23.9 14.4 18.3 39.0 ## 7 Balthasar time1 17.2 23.9 14.4 18.3 39.0 ## 8 Balthasar time2 17.2 23.9 14.4 18.3 39.0 ## 9 Balthasar time3 42.9 23.9 14.4 18.3 39.0 ## 10 Claudia time1 12.8 23.9 14.4 18.3 39.0 ## 11 Claudia time2 20.3 23.9 14.4 18.3 39.0 ## 12 Claudia time3 33.2 23.9 14.4 18.3 39.0 ## 13 Heike time1 14.9 23.9 14.4 18.3 39.0 ## 14 Heike time2 25.3 23.9 14.4 18.3 39.0 ## 15 Heike time3 58.3 23.9 14.4 18.3 39.0 ## 16 Marion time1 11.1 23.9 14.4 18.3 39.0 ## 17 Marion time2 12.1 23.9 14.4 18.3 39.0 ## 18 Marion time3 39.1 23.9 14.4 18.3 39.0 Next steps: We want the square sum of errors for the entire model. Lets do this step by step: d &lt;- d %&gt;% mutate( grand_mean_diff = (time - grand_mean)^2, sum_sq_total = sum(grand_mean_diff), ) %&gt;% mutate( # differences for each factor level # grand mean differences diff_time1 = mean_run1 - grand_mean, diff_time2 = mean_run2 - grand_mean, diff_time3 = mean_run3 - grand_mean, # square the differences and multiply by n Diff_time1 = (diff_time1^2) * 6, Diff_time2 = (diff_time2^2) * 6, Diff_time3 = (diff_time3^2) * 6, # and add them all up sum_sq_model = Diff_time1 + Diff_time2 + Diff_time3 ) d ## # A tibble: 18 × 16 ## student when time grand_mean mean_run1 mean_run2 mean_run3 grand_mean_diff sum_sq_total diff_time1 diff_time2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raphael time1 14.3 23.9 14.4 18.3 39.0 92.8 2815. -9.52 -5.6 ## 2 Raphael time2 19.2 23.9 14.4 18.3 39.0 22.4 2815. -9.52 -5.6 ## 3 Raphael time3 27.3 23.9 14.4 18.3 39.0 11.3 2815. -9.52 -5.6 ## 4 Nicolaus time1 16.2 23.9 14.4 18.3 39.0 59.8 2815. -9.52 -5.6 ## 5 Nicolaus time2 15.9 23.9 14.4 18.3 39.0 64.5 2815. -9.52 -5.6 ## 6 Nicolaus time3 33.5 23.9 14.4 18.3 39.0 91.5 2815. -9.52 -5.6 ## 7 Balthasar time1 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 8 Balthasar time2 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 9 Balthasar time3 42.9 23.9 14.4 18.3 39.0 360. 2815. -9.52 -5.6 ## 10 Claudia time1 12.8 23.9 14.4 18.3 39.0 124. 2815. -9.52 -5.6 ## 11 Claudia time2 20.3 23.9 14.4 18.3 39.0 13.2 2815. -9.52 -5.6 ## 12 Claudia time3 33.2 23.9 14.4 18.3 39.0 85.9 2815. -9.52 -5.6 ## 13 Heike time1 14.9 23.9 14.4 18.3 39.0 81.6 2815. -9.52 -5.6 ## 14 Heike time2 25.3 23.9 14.4 18.3 39.0 1.87 2815. -9.52 -5.6 ## 15 Heike time3 58.3 23.9 14.4 18.3 39.0 1181. 2815. -9.52 -5.6 ## 16 Marion time1 11.1 23.9 14.4 18.3 39.0 165. 2815. -9.52 -5.6 ## 17 Marion time2 12.1 23.9 14.4 18.3 39.0 140. 2815. -9.52 -5.6 ## 18 Marion time3 39.1 23.9 14.4 18.3 39.0 230. 2815. -9.52 -5.6 ## diff_time3 Diff_time1 Diff_time2 Diff_time3 sum_sq_model ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 15.1 543. 188. 1371. 2103. ## 2 15.1 543. 188. 1371. 2103. ## 3 15.1 543. 188. 1371. 2103. ## 4 15.1 543. 188. 1371. 2103. ## 5 15.1 543. 188. 1371. 2103. ## 6 15.1 543. 188. 1371. 2103. ## 7 15.1 543. 188. 1371. 2103. ## 8 15.1 543. 188. 1371. 2103. ## 9 15.1 543. 188. 1371. 2103. ## 10 15.1 543. 188. 1371. 2103. ## 11 15.1 543. 188. 1371. 2103. ## 12 15.1 543. 188. 1371. 2103. ## 13 15.1 543. 188. 1371. 2103. ## 14 15.1 543. 188. 1371. 2103. ## 15 15.1 543. 188. 1371. 2103. ## 16 15.1 543. 188. 1371. 2103. ## 17 15.1 543. 188. 1371. 2103. ## 18 15.1 543. 188. 1371. 2103. And now we need the square sum of the errors (sorry for switching to base R here, I have not found a satisfying way of doing this with dplyr). Again, we want to see how far the individual times deviate from the the respective condition means: d$error_diff[d$when == &quot;time1&quot;] &lt;- (d$time[d$when == &quot;time1&quot;] - d$mean_run1[d$when == &quot;time1&quot;])^2 ## Warning: Unknown or uninitialised column: `error_diff`. d$error_diff[d$when == &quot;time2&quot;] &lt;- (d$time[d$when == &quot;time2&quot;] - d$mean_run2[d$when == &quot;time2&quot;])^2 d$error_diff[d$when == &quot;time3&quot;] &lt;- (d$time[d$when == &quot;time3&quot;] - d$mean_run3[d$when == &quot;time3&quot;])^2 d$sum_sq_error &lt;- sum(d$error_diff) Now let’s check whether the total sum of squares is made up of the sums of squares of the model and the errors: mean(d$sum_sq_total) == mean(d$sum_sq_model) + mean(d$sum_sq_error) ## [1] TRUE Next step: degrees of freedom. d &lt;- d %&gt;% mutate( df_total = length(time) - 1, df_model = length(unique(when)) - 1, df_error = length(time) - length(unique(when)) ) d ## # A tibble: 18 × 21 ## student when time grand_mean mean_run1 mean_run2 mean_run3 grand_mean_diff sum_sq_total diff_time1 diff_time2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raphael time1 14.3 23.9 14.4 18.3 39.0 92.8 2815. -9.52 -5.6 ## 2 Raphael time2 19.2 23.9 14.4 18.3 39.0 22.4 2815. -9.52 -5.6 ## 3 Raphael time3 27.3 23.9 14.4 18.3 39.0 11.3 2815. -9.52 -5.6 ## 4 Nicolaus time1 16.2 23.9 14.4 18.3 39.0 59.8 2815. -9.52 -5.6 ## 5 Nicolaus time2 15.9 23.9 14.4 18.3 39.0 64.5 2815. -9.52 -5.6 ## 6 Nicolaus time3 33.5 23.9 14.4 18.3 39.0 91.5 2815. -9.52 -5.6 ## 7 Balthasar time1 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 8 Balthasar time2 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 9 Balthasar time3 42.9 23.9 14.4 18.3 39.0 360. 2815. -9.52 -5.6 ## 10 Claudia time1 12.8 23.9 14.4 18.3 39.0 124. 2815. -9.52 -5.6 ## 11 Claudia time2 20.3 23.9 14.4 18.3 39.0 13.2 2815. -9.52 -5.6 ## 12 Claudia time3 33.2 23.9 14.4 18.3 39.0 85.9 2815. -9.52 -5.6 ## 13 Heike time1 14.9 23.9 14.4 18.3 39.0 81.6 2815. -9.52 -5.6 ## 14 Heike time2 25.3 23.9 14.4 18.3 39.0 1.87 2815. -9.52 -5.6 ## 15 Heike time3 58.3 23.9 14.4 18.3 39.0 1181. 2815. -9.52 -5.6 ## 16 Marion time1 11.1 23.9 14.4 18.3 39.0 165. 2815. -9.52 -5.6 ## 17 Marion time2 12.1 23.9 14.4 18.3 39.0 140. 2815. -9.52 -5.6 ## 18 Marion time3 39.1 23.9 14.4 18.3 39.0 230. 2815. -9.52 -5.6 ## diff_time3 Diff_time1 Diff_time2 Diff_time3 sum_sq_model error_diff sum_sq_error df_total df_model df_error ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 15.1 543. 188. 1371. 2103. 0.0136 712. 17 2 15 ## 2 15.1 543. 188. 1371. 2103. 0.751 712. 17 2 15 ## 3 15.1 543. 188. 1371. 2103. 138. 712. 17 2 15 ## 4 15.1 543. 188. 1371. 2103. 3.18 712. 17 2 15 ## 5 15.1 543. 188. 1371. 2103. 5.92 712. 17 2 15 ## 6 15.1 543. 188. 1371. 2103. 30.8 712. 17 2 15 ## 7 15.1 543. 188. 1371. 2103. 7.75 712. 17 2 15 ## 8 15.1 543. 188. 1371. 2103. 1.28 712. 17 2 15 ## 9 15.1 543. 188. 1371. 2103. 14.8 712. 17 2 15 ## 10 15.1 543. 188. 1371. 2103. 2.61 712. 17 2 15 ## 11 15.1 543. 188. 1371. 2103. 3.87 712. 17 2 15 ## 12 15.1 543. 188. 1371. 2103. 34.2 712. 17 2 15 ## 13 15.1 543. 188. 1371. 2103. 0.234 712. 17 2 15 ## 14 15.1 543. 188. 1371. 2103. 48.5 712. 17 2 15 ## 15 15.1 543. 188. 1371. 2103. 371. 712. 17 2 15 ## 16 15.1 543. 188. 1371. 2103. 11.0 712. 17 2 15 ## 17 15.1 543. 188. 1371. 2103. 38.9 712. 17 2 15 ## 18 15.1 543. 188. 1371. 2103. 0.00250 712. 17 2 15 # sanity check here as well: d$df_total == d$df_model + d$df_error ## [1] TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE And now we get to the meat of the ANOVA: the mean sums of squares d &lt;- d %&gt;% mutate( mean_sum_squares_model = mean(sum_sq_model) / df_model, mean_sum_squares_error = mean(sum_sq_error) / df_error ) d ## # A tibble: 18 × 23 ## student when time grand_mean mean_run1 mean_run2 mean_run3 grand_mean_diff sum_sq_total diff_time1 diff_time2 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Raphael time1 14.3 23.9 14.4 18.3 39.0 92.8 2815. -9.52 -5.6 ## 2 Raphael time2 19.2 23.9 14.4 18.3 39.0 22.4 2815. -9.52 -5.6 ## 3 Raphael time3 27.3 23.9 14.4 18.3 39.0 11.3 2815. -9.52 -5.6 ## 4 Nicolaus time1 16.2 23.9 14.4 18.3 39.0 59.8 2815. -9.52 -5.6 ## 5 Nicolaus time2 15.9 23.9 14.4 18.3 39.0 64.5 2815. -9.52 -5.6 ## 6 Nicolaus time3 33.5 23.9 14.4 18.3 39.0 91.5 2815. -9.52 -5.6 ## 7 Balthasar time1 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 8 Balthasar time2 17.2 23.9 14.4 18.3 39.0 45.3 2815. -9.52 -5.6 ## 9 Balthasar time3 42.9 23.9 14.4 18.3 39.0 360. 2815. -9.52 -5.6 ## 10 Claudia time1 12.8 23.9 14.4 18.3 39.0 124. 2815. -9.52 -5.6 ## 11 Claudia time2 20.3 23.9 14.4 18.3 39.0 13.2 2815. -9.52 -5.6 ## 12 Claudia time3 33.2 23.9 14.4 18.3 39.0 85.9 2815. -9.52 -5.6 ## 13 Heike time1 14.9 23.9 14.4 18.3 39.0 81.6 2815. -9.52 -5.6 ## 14 Heike time2 25.3 23.9 14.4 18.3 39.0 1.87 2815. -9.52 -5.6 ## 15 Heike time3 58.3 23.9 14.4 18.3 39.0 1181. 2815. -9.52 -5.6 ## 16 Marion time1 11.1 23.9 14.4 18.3 39.0 165. 2815. -9.52 -5.6 ## 17 Marion time2 12.1 23.9 14.4 18.3 39.0 140. 2815. -9.52 -5.6 ## 18 Marion time3 39.1 23.9 14.4 18.3 39.0 230. 2815. -9.52 -5.6 ## diff_time3 Diff_time1 Diff_time2 Diff_time3 sum_sq_model error_diff sum_sq_error df_total df_model df_error ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 15.1 543. 188. 1371. 2103. 0.0136 712. 17 2 15 ## 2 15.1 543. 188. 1371. 2103. 0.751 712. 17 2 15 ## 3 15.1 543. 188. 1371. 2103. 138. 712. 17 2 15 ## 4 15.1 543. 188. 1371. 2103. 3.18 712. 17 2 15 ## 5 15.1 543. 188. 1371. 2103. 5.92 712. 17 2 15 ## 6 15.1 543. 188. 1371. 2103. 30.8 712. 17 2 15 ## 7 15.1 543. 188. 1371. 2103. 7.75 712. 17 2 15 ## 8 15.1 543. 188. 1371. 2103. 1.28 712. 17 2 15 ## 9 15.1 543. 188. 1371. 2103. 14.8 712. 17 2 15 ## 10 15.1 543. 188. 1371. 2103. 2.61 712. 17 2 15 ## 11 15.1 543. 188. 1371. 2103. 3.87 712. 17 2 15 ## 12 15.1 543. 188. 1371. 2103. 34.2 712. 17 2 15 ## 13 15.1 543. 188. 1371. 2103. 0.234 712. 17 2 15 ## 14 15.1 543. 188. 1371. 2103. 48.5 712. 17 2 15 ## 15 15.1 543. 188. 1371. 2103. 371. 712. 17 2 15 ## 16 15.1 543. 188. 1371. 2103. 11.0 712. 17 2 15 ## 17 15.1 543. 188. 1371. 2103. 38.9 712. 17 2 15 ## 18 15.1 543. 188. 1371. 2103. 0.00250 712. 17 2 15 ## mean_sum_squares_model mean_sum_squares_error ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1051. 47.5 ## 2 1051. 47.5 ## 3 1051. 47.5 ## 4 1051. 47.5 ## 5 1051. 47.5 ## 6 1051. 47.5 ## 7 1051. 47.5 ## 8 1051. 47.5 ## 9 1051. 47.5 ## 10 1051. 47.5 ## 11 1051. 47.5 ## 12 1051. 47.5 ## 13 1051. 47.5 ## 14 1051. 47.5 ## 15 1051. 47.5 ## 16 1051. 47.5 ## 17 1051. 47.5 ## 18 1051. 47.5 Finally, by diving systematic variance (contributed by our experimental manipulation) by unsystematic variance (contributed by randomness), we get our \\(F\\) value: f_value &lt;- mean(d$mean_sum_squares_model) / mean(d$mean_sum_squares_error) f_value ## [1] 22.133813 And now let’s see whether R comes up with the same results: summary(aov(time ~ when, data = d)) ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## when 2 2102.64 1051.3 22.134 0.00003346 *** ## Residuals 15 712.48 47.5 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 summary(lm(time ~ when, data = d)) ## ## Call: ## lm(formula = time ~ when, data = d) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.75000 -3.09583 -0.03333 1.92083 19.25000 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 14.4167 2.8136 5.1239 0.0001247 *** ## whentime2 3.9167 3.9790 0.9843 0.3405686 ## whentime3 24.6333 3.9790 6.1908 0.00001728 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 6.8919 on 15 degrees of freedom ## Multiple R-squared: 0.74691, Adjusted R-squared: 0.71317 ## F-statistic: 22.134 on 2 and 15 DF, p-value: 0.000033462 Question: Why can we not use the t.test() here? 3.7 Interpreting an ANOVA And now let’s talk about interpretation: Here is a situation where we have one independent variable (our model would be of the one-factor kind) but it has 4 levels: Figure 3.3: One factorial design (four factor levels). Question: Suppose that our one-way ANOVA indicates that we have a main effect in the situation above. What does that mean? How would you interpret the main effect? 3.8 Wrap-Up Today we talked about the common core between a number of statistical tests. In particular, we saw that the t-test, the ANOVA, and the LM are equivalent in the simple, one-factor-two-levels cases (without repeated measures). So, in fact, while we talked about different tests, you actually learned one statistical approach: line fitting by minimizing variance. Then we talked about repeated measures, as a systematic portion of variance that is unrelated to our experimental manipulation. Instead, it is systematically associated with a random portion of our experiments, namely the sampling process. Importantly, in psycholinguistics we often have two instances of repeated measures: the participants and the items. We saw a way of dealing with this by computing two ANOVAs. But this is not very satisfying since we would like a model that can deal with both repeated measures components at the same time. We will see one at the end of the class. Finally, we saw how to compute a two-way ANOVA, namely a model where we are interested in two factors at the same time (as well as their interaction). References "],["4-regression.html", "4 Regression 4.1 Models and regression lines 4.2 Varying intercepts, slopes, and errors 4.3 Regression in R 4.4 Multiple Regression 4.5 Centering and Standardization 4.6 Interpreting interactions", " 4 Regression 4.1 Models and regression lines As we saw yesterday, ANOVAs and the LM are really close to each other. Both can be conceptualized as linear models that do most of their work by fitting a line. We talked about this briefly yesterday, today we should dive into the theory a little bit more. As you may remember from school, we can describe a line as follows, where \\(b_0\\) is the intercept, \\(b_1\\) the slope, and \\(e\\) is the error term: \\[y = b_0 + b_1x + e\\] But let us take a step back first. Say this is our data: But this is just a representation, not a model. For example, we could have a so-called intercept-only model. In these cases, the intercept is just the grand mean we saw in the ANOVA by hand portion yesterday: \\[y_i = \\bar{y} + e_i\\] This model just says that our line maps each individual observation \\(y_i\\) to the grand mean, \\(\\bar{y}\\) plus some error, where the error is the distance from the observation to the mean: summary(lm(judgment ~ 1, data = d_psp)) ## ## Call: ## lm(formula = judgment ~ 1, data = d_psp) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.64688 -1.64688 0.35312 1.35312 1.35312 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.646880 0.056258 64.824 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.442 on 656 degrees of freedom In a plot, the line looks like this: But, of course, the mean is not a really interesting model. In what is to come, we will have a look at more complex models and talk about how to interpret them. 4.2 Varying intercepts, slopes, and errors Let’s familiarize ourselves with the graphical effects of changing intercepts, slopes, and errors. As a first step, we manipulate only the intercepts of the lines, while keeping everything else constant. That is, the slope of the line should remain exactly the same. eq_0 &lt;- function(x) { 0 + 1 * x } eq_1 &lt;- function(x) { 1 + 1 * x } eq_2 &lt;- function(x) { 2 + 1 * x } base_plot &lt;- function(eq) { tibble(x = c(-7, 7)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = eq, color = colors[1]) + coord_cartesian(ylim = c(0, 7)) + geom_vline(xintercept = 0, color = &quot;grey&quot;, lty = &quot;dashed&quot;) } p_0 &lt;- base_plot(eq_0) p_1 &lt;- base_plot(eq_1) p_2 &lt;- base_plot(eq_2) p_0 / p_1 / p_2 Figure 4.1: Varying slopes. Below, I keep the intercepts constant (at \\(0\\)) and vary only the slopes: eq_0 &lt;- function(x) { 0 + 0 * x } eq_1 &lt;- function(x) { 0 + 1 * x } eq_2 &lt;- function(x) { 0 + 2 * x } base_plot &lt;- function(eq) { tibble(x = c(-7, 7)) %&gt;% ggplot(aes(x = x)) + stat_function(fun = eq, color = colors[1]) + coord_cartesian(ylim = c(0, 7)) + geom_vline(xintercept = 0, color = &quot;grey&quot;, lty = &quot;dashed&quot;) } p_0 &lt;- base_plot(eq_0) p_1 &lt;- base_plot(eq_1) p_2 &lt;- base_plot(eq_2) p_0 / p_1 / p_2 Figure 4.2: Varying slopes. And here I vary the errors. In the first plot, \\(x\\) predicts \\(y\\) perfectly, in the second, there is some noise. set.seed(1234) base_plot &lt;- function(error) { x &lt;- rnorm(50) y &lt;- 5 + 3 * x + error tibble(x = x, y = y) %&gt;% ggplot(aes(x = x, y = y)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, color = colors[1]) + geom_point(size = 3) } p_0 &lt;- base_plot(rep(0, 50)) p_1 &lt;- base_plot(rnorm(50)) p_0 / p_1 Figure 4.3: Varying error terms. Note that the errors are also a part of the unsystematic variance we talked about. They represent deviations from our prediction that cannot be explained by our model. These errors, as said before, are also called residuals. 4.3 Regression in R Here is some data from this paper. students &lt;- read.csv(here(&quot;assets&quot;, &quot;data&quot;, &quot;student-mat.csv&quot;)) %&gt;% select(G3, sex, age, studytime) head(students) ## G3 sex age studytime ## 1 6 F 18 2 ## 2 6 F 17 2 ## 3 10 F 15 2 ## 4 15 F 15 3 ## 5 10 F 16 2 ## 6 15 M 16 2 Let’s again compute a \\(t\\)-test to see whether there are any differences on the G3 scores by sex: t.test(G3 ~ sex, data = students, paired = FALSE, var.equal = TRUE) %&gt;% tidy() ## # A tibble: 1 × 10 ## estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high method alternative ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 -0.948 9.97 10.9 -2.06 0.0399 393 -1.85 -0.0441 Two Sample t-test two.sided Once again, in this simple case, the \\(t\\)-test is equivalent to the linear regression (called using the lm() function): m_students &lt;- lm(G3 ~ sex, data = students) summary(m_students) %&gt;% tidy() %&gt;% mutate(p.value = scales::pvalue(p.value)) ## # A tibble: 2 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 (Intercept) 9.97 0.316 31.5 &lt;0.001 ## 2 sexM 0.948 0.460 2.06 0.040 The line intercepts the \\(y\\) axis at \\(\\sim9.9\\). As you change the \\(x\\)-axis value by \\(1\\) (which represents our change from female to male because of the way the predictor was represented), the slope of the line changes by \\(\\sim 0.94\\). In effect, everything else being equal, male students have higher G3 scores than women. But let’s also check if we can see the coefficients that the lm() computed in a plot. To generate the line, I use the geom_smooth() (with method = \"lm\") function: students %&gt;% mutate(sex = if_else(sex == &quot;F&quot;, 0, 1)) %&gt;% ggplot(aes(x = sex, y = G3)) + geom_jitter(alpha = .2) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2, color = colors[1]) + stat_smooth(method = &quot;lm&quot;, geom = &quot;line&quot;, color = colors[1], se = FALSE, fullrange = TRUE) + scale_y_continuous(breaks = scales::breaks_extended(n = 18)) + scale_x_continuous(breaks = c(0, 1)) As we can (hopefully) see, the intercept that the model predicts is exactly the mean for the women (coded here 0, more on that later). And if we subtract the condition mean of the men with that of the women, we are left with the predicted slope. students %&gt;% group_by(sex) %&gt;% summarise( mean = mean(G3) ) ## # A tibble: 2 × 2 ## sex mean ## &lt;chr&gt; &lt;dbl&gt; ## 1 F 9.97 ## 2 M 10.9 So, essentially what we do when we use the tests we’ve looked at so far is line fitting. We will walk trough a few other cases to see what’s going on when we increase the complexity. 4.4 Multiple Regression In multiple regression, we simply add more slopes, while the intercept is now dependent on multiple variables: \\[y = b_0 + b_1x + b_2x + ... + e\\] Here is the student data again: students_mult &lt;- lm(G3 ~ age + sex, data = students) summary(students_mult) ## ## Call: ## lm(formula = G3 ~ age + sex, data = students) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.85931 -1.85931 0.28063 3.14069 9.75707 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.50235 2.99661 6.5081 0.0000000002329 *** ## age -0.56997 0.17813 -3.1997 0.001488 ** ## sexM 0.90648 0.45467 1.9937 0.046877 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.51 on 392 degrees of freedom ## Multiple R-squared: 0.035884, Adjusted R-squared: 0.030965 ## F-statistic: 7.295 on 2 and 392 DF, p-value: 0.00077507 Let’s plot the data and see what the lines look like. Below I explicitly extract the coefficients from the lm object for illustrative purposes: equation1 &lt;- function(x) { coef(students_mult)[2] * x + coef(students_mult)[1] } equation2 &lt;- function(x) { coef(students_mult)[2] * x + coef(students_mult)[1] + coef(students_mult)[3] } students %&gt;% ggplot(aes( x = age, y = G3, color = sex, group = sex, pch = sex )) + geom_jitter(alpha = .2) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2) + stat_function( fun = equation1, geom = &quot;line&quot;, color = colors[1] ) + stat_function( fun = equation2, geom = &quot;line&quot;, color = colors[2] ) + xlim(0, 25) + scale_y_continuous(breaks = scales::breaks_extended(n = 18)) + scale_color_manual(values = colors) And now let’s add an interaction term: students_mult_int &lt;- lm(G3 ~ age * sex, data = students) summary(students_mult_int) ## ## Call: ## lm(formula = G3 ~ age * sex, data = students) ## ## Residuals: ## Min 1Q Median 3Q Max ## -12.40927 -1.70410 0.39417 2.89459 10.09934 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 13.15019 4.35878 3.0169 0.00272 ** ## age -0.19030 0.25986 -0.7323 0.46441 ## sexM 12.78493 5.95646 2.1464 0.03246 * ## age:sexM -0.71142 0.35571 -2.0000 0.04619 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.4928 on 391 degrees of freedom ## Multiple R-squared: 0.045647, Adjusted R-squared: 0.038325 ## F-statistic: 6.2339 on 3 and 391 DF, p-value: 0.00038297 students %&gt;% ggplot(aes(x = age, y = G3, color = sex, pch = sex, group = sex)) + geom_jitter(alpha = .2) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2) + stat_smooth(method = &quot;lm&quot;, fullrange = TRUE, se = FALSE) + scale_y_continuous(breaks = scales::breaks_extended(n = 16)) + xlim(0, 25) + scale_color_manual(values = colors) Here is an example equation for a regression with two predictors and an interaction term (without the error): \\[y = b_0 + b_1x_1 + b_2x_2 + b_3(x_1 * x_2)\\] As you see here, the interaction term estimates a multiplicative effect for the two predictors. As with the main effects, if \\(b_3\\) is close to zero, the interaction is small. 4.5 Centering and Standardization Here is a small illustration of scaling/transforming, centering, and standardization. Centering simply involves subtracting the mean from the individual observations. This will give a new meaning to the intercept, which will now represent the overall mean of the dependent variable, rather than its value at predictor = 0. In formula-talk, we could represent a centered predictor like this: \\[y = b_0 + b_1(x - \\bar{x}) + e\\] Let’s what happens when we center the age predictor below. students &lt;- students %&gt;% mutate( age_bar = mean(age), age_c = age - age_bar # alternative: scale(., scale = FALSE) ) students %&gt;% select(G3, age, age_c, age_bar) %&gt;% head() ## G3 age age_c age_bar ## 1 6 18 1.30379747 16.696203 ## 2 6 17 0.30379747 16.696203 ## 3 10 15 -1.69620253 16.696203 ## 4 15 15 -1.69620253 16.696203 ## 5 10 16 -0.69620253 16.696203 ## 6 15 16 -0.69620253 16.696203 students_c &lt;- lm(G3 ~ age_c, data = students) summary(students_c) ## ## Call: ## lm(formula = G3 ~ age_c, data = students) ## ## Residuals: ## Min 1Q Median 3Q Max ## -11.39920 -1.65882 0.34118 3.18092 9.50143 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 10.41519 0.22778 45.7253 &lt; 0.00000000000000022 *** ## age_c -0.58013 0.17873 -3.2458 0.001271 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.527 on 393 degrees of freedom ## Multiple R-squared: 0.026108, Adjusted R-squared: 0.02363 ## F-statistic: 10.535 on 1 and 393 DF, p-value: 0.0012714 students %&gt;% mutate(sex = if_else(sex == &quot;F&quot;, 0, 1)) %&gt;% ggplot(aes(x = age_c, y = G3)) + geom_jitter(alpha = .2) + stat_summary(fun = mean, geom = &quot;point&quot;, size = 2, color = colors[1]) + stat_smooth(method = &quot;lm&quot;, geom = &quot;line&quot;, color = colors[1], se = FALSE, fullrange = TRUE) + scale_y_continuous(breaks = scales::breaks_extended(n = 18)) Question: Why does it make sense in this case to center the age predictor? Does it help interpreting the model at all? Why? We call centering a linear transformation because all values are affected in the same way. Adding 2 to every value would also be linear, for example. Here’s another linear transformation, the so-called \\(z\\) transformation. What we do here is to take the centered value and divide it by the standard deviation of the centered variable. This turns all of our values into ones from a distribution with \\(\\bar{x} \\approx 0\\) and \\(s = 1\\). students &lt;- students %&gt;% mutate( age_z = age_c / sd(age_c), # alternative: scale(.) age_z_bar = mean(age_z), age_z_s = sd(age_z) ) students %&gt;% select(G3, age, age_c, age_bar, age_z, age_z_bar, age_z_s) %&gt;% head() ## G3 age age_c age_bar age_z age_z_bar age_z_s ## 1 6 18 1.30379747 16.696203 1.02175064 0.0000000000000012813074 1 ## 2 6 17 0.30379747 16.696203 0.23807782 0.0000000000000012813074 1 ## 3 10 15 -1.69620253 16.696203 -1.32926782 0.0000000000000012813074 1 ## 4 15 15 -1.69620253 16.696203 -1.32926782 0.0000000000000012813074 1 ## 5 10 16 -0.69620253 16.696203 -0.54559500 0.0000000000000012813074 1 ## 6 15 16 -0.69620253 16.696203 -0.54559500 0.0000000000000012813074 1 Question: What could be the upshot of such an operation? Maybe try to think of predictors other than age. While a log transformation is non-linear because it affects larger values more strongly than it does smaller ones (and thus serves to cure positive skew), centering and standardization are both linear transformations that affect all observations in the same way. range(diamonds$price) ## [1] 326 18823 diamonds_scaled &lt;- diamonds %&gt;% select(carat, price) %&gt;% mutate( price_log10 = log10(price), price_log10_c = price_log10 - mean(price_log10), # or: scale(., scale = F) price_log10_z = price_log10_c / sd(price_log10_c) # or: scale(.) ) diamonds_scaled %&gt;% ggplot() + geom_histogram(aes(x = price), fill = colors[1]) + diamonds_scaled %&gt;% ggplot() + geom_histogram(aes(x = price_log10), fill = colors[1]) + diamonds_scaled %&gt;% ggplot() + geom_histogram(aes(x = price_log10_c), fill = colors[1]) + diamonds_scaled %&gt;% ggplot() + geom_histogram(aes(x = price_log10_z), fill = colors[1]) Figure 4.4: Comparison of raw prices with scaled and transformed values. Note that the output of the Pearson correlation function cor is exactly identical to the slope estimated by a linear regression with one predictor when both \\(y\\) and \\(x\\) are z-scores: diamonds_scaled &lt;- diamonds_scaled %&gt;% mutate( carat_log10_z = scale(log10(carat)) ) cor(diamonds_scaled$price_log10_z, diamonds_scaled$carat_log10_z) ## [,1] ## [1,] 0.96591372 # -1 removes the intercept computation, we know it&#39;s zero because of z-scores lm(price_log10_z ~ -1 + carat_log10_z, data = diamonds_scaled) ## ## Call: ## lm(formula = price_log10_z ~ -1 + carat_log10_z, data = diamonds_scaled) ## ## Coefficients: ## carat_log10_z ## 0.96591 In the plots below, we have some interactions: Above: main effect possibilities without interactions below: including interactions – main effects determined by means, interactions by slopes. Ltr/ttb: no main effect, main effect for \\(x\\) factor, split factor main effect, main effects for both, no main effect but interaction, \\(x\\) factor effect plus interaction, split factor effect plus interaction, two main effects plus interaction. Figure 4.5: Interactions 4.6 Interpreting interactions Interactions make interpretation hard. Let’s see what there is to do. Here is the simple pricing model with two predictors. The first is continuous, and gives us as indication about the weight of the diamonds. The second is categorical and gives us color information. Here, we use treatment contrasts. diamonds_sub &lt;- diamonds %&gt;% select(carat, price, color) %&gt;% filter(color %in% c(&quot;E&quot;, &quot;J&quot;)) # this does not do much here, because R automatically uses treatment coding # but it helps to be explicit about this diamonds_sub$color &lt;- droplevels(diamonds_sub$color) contrasts(diamonds_sub$color) &lt;- contr.treatment(2) lm(price ~ carat + color, diamonds_sub) ## ## Call: ## lm(formula = price ~ carat + color, data = diamonds_sub) ## ## Coefficients: ## (Intercept) carat color2 ## -2042.5 7781.5 -1676.9 Let’s clean up the output of the lm() command a bit. The tidy() function from the broom package gives us a dataframe, rather than the onslaught of text we saw above. This is a little bit nicer to read: # load the broom package library(broom) lm_diam_twopred &lt;- lm(price ~ carat + color, diamonds_sub) lm_diam_twopred %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 3 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -2042. ## 2 carat 7782. ## 3 color2 -1677. Let’s compare this to a model with an interaction term (where we replace + with * in the formula of the the lm() function call): lm_diam_twopredint &lt;- lm(price ~ carat * color, diamonds_sub) lm_diam_twopredint %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) -2381. ## 2 carat 8296. ## 3 color2 -540. ## 4 carat:color2 -1202. So, the intercepts and slopes differ. Why is that? First, their meanings changed. Recall that in the two-predictor case the intercept is to be interpreted as 0 carat and E color. The slope for carat tells us the change in price for one carat. The slope for the treatment (our color J) then tells us that this color achieves a lower price per change in carat. With the interaction present, these meanings change slightly. The carat slope is now the slope only for the color E, and not the average effect of weight. Similarly, the effect of color.L now only tracks the color difference for diamonds with a weight of 0 carat. It is no longer the average effect of the color change. And now for the new coefficient: the interaction term. This tells us how to adjust the slopes for the color L. Here, because of the negative sign, this means that for the color L the slope is less steep than it is for our reference level. This is what we call a simple effect: namely the influence of one predictor for only one level of another predictor. This is different from the main effects we got before. Note also that for the case with two predictors with two levels each (and an interaction), each of the two predictors will be associated with two simple effects, namely one for each value of the other predictor. And the interaction represents the difference between these simple effects. If the interaction term is zero, the two simple effects are equal (and equal to the main effect). The mental calculation we have to do is this: The carat slope for the other color is \\(8296\\). To get the slope for color L, we need to add the interaction term: \\(8296 + (- 1202) = 7094\\). That is, this color is less affected by an increase in weight, and thus achieves a lower price overall. We can see this in the plots below as well (note that the individual data points do not change, only the representation of the data in the form of a line does; our model is different): (#fig:p_twopredint)Comparison between (i) no interaction term and (ii) yes interaction term. Note that we can also add a further technique to make the output of the interaction term a little bit easier: We can center the carat predictor. Recall that this changes the meaning of the intercept to the mean. This then means that we have a changed interpretation of the color manipulation as well: It represents the difference between the two for the average value of carat in our dataset. Arguably, this is easier to understand than thinking about diamonds with a weight of zero (the interpretation in our uncentered model). Let’s look at this: lm_diam_twopredint_c &lt;- diamonds_sub %&gt;% mutate(carat_c = carat - mean(carat)) %&gt;% lm(price ~ carat_c * color, .) lm_diam_twopredint_c %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 4009. ## 2 carat_c 8296. ## 3 color2 -1465. ## 4 carat_c:color2 -1202. Another way of going about this is to change the contast coding from treatment conding to sum coding. If we switch to sum coding, each coefficient for the sum-coded predictors represents the half of the difference between the two predictor levels. In other words, what we now have are main effects, rather than simple effects. You can think of sum-coding for categorical variables as the analogue of centering (or standardizing, which subsumes centering) for continuous predictors. diamonds_sub$color &lt;- droplevels(diamonds_sub$color) contrasts(diamonds_sub$color) &lt;- contr.sum(2) diamonds_sub %&gt;% mutate(carat_c = carat - mean(carat)) %&gt;% lm(price ~ carat_c * color, data = .) %&gt;% tidy() %&gt;% select(term, estimate) ## # A tibble: 4 × 2 ## term estimate ## &lt;chr&gt; &lt;dbl&gt; ## 1 (Intercept) 3276. ## 2 carat_c 7695. ## 3 color1 733. ## 4 carat_c:color1 601. Exercise: So far, we looked at the mixed cases, where one predictor is categorical and one is continuous. As an exercise, please compute (at least) two further linear models, each with two predictors of the same data type. In the case with two continuous predictors, you should think about centering or standardizing the predictors and try to think about the changes this choice induces. There’s no need to do an interaction analysis. For the case with two categorical predictors, please include an interaction term in the model formula. You should also make a decision about what contrast coding scheme you want to follow. For both, you should write a short description of the model you ran as well an interpretation of the modal coefficients. "],["5-linear-mixed-models.html", "5 Linear Mixed Models 5.1 Previously … 5.2 Before we go on, let’s talk about pooling 5.3 Random Effects 5.4 What do I do about my \\(p\\)-values?", " 5 Linear Mixed Models 5.1 Previously … In the last days, we talked about linear models in various instances, like the \\(t\\)-test, the linear model, and the ANOVA. For the more complex experimental data that we looked at, we found out that the \\(t\\)-test just doesn’t cut it. Then, we also saw there are variance components that are unsystematic from the perspective of our experiment, but which are nonetheless systematic per se. We saw two cases of these repeated measures with the ANOVA: one related to participants (when a factor is within participants) and one related to items (when a manipulation occurs within items). With the ANOVA, it was not possible to have a single model that captured both sources of repeated measures at the same time. Today, we will have a look at a class of models that can do this. These are called (linear) mixed models. In the context of these models, these sources of variance are called random effects, because they are related to non-experimental variables. Let’s first develop an intuition what these models do with the random effects before we dive in deeper. For familiarity purposes, we will use the data from Chen et al. (2022) again. d_psp &lt;- read_csv(here(&quot;assets&quot;, &quot;data&quot;, &quot;psp-data.csv&quot;)) %&gt;% filter(trigger_cat != &quot;appo&quot;, stage != &quot;Children&quot;) %&gt;% select(id, itemid, trigger_cat, issue, judgment) d_psp &lt;- d_psp %&gt;% mutate(trigger_cat_d = if_else(trigger_cat == &quot;hard&quot;, 0, 1)) head(d_psp) ## # A tibble: 6 × 6 ## id itemid trigger_cat issue judgment trigger_cat_d ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auncqkpvl6b3rp0d3g833vmuh6 gewinnen2 soft at-issue 5 1 ## 2 auncqkpvl6b3rp0d3g833vmuh6 schaffen1 soft at-issue 1 1 ## 3 auncqkpvl6b3rp0d3g833vmuh6 auch3 hard at-issue 1 0 ## 4 auncqkpvl6b3rp0d3g833vmuh6 entdecken1 soft at-issue 5 1 ## 5 auncqkpvl6b3rp0d3g833vmuh6 schaffen3 soft at-issue 1 1 ## 6 auncqkpvl6b3rp0d3g833vmuh6 cleft3 hard non-at-issue 5 0 5.2 Before we go on, let’s talk about pooling # complete pooling d_psp %&gt;% lm(judgment ~ 1, data = .) %&gt;% tidy() ## # A tibble: 1 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 3.65 0.0563 64.8 2.02e-287 # no pooling d_psp %&gt;% mutate(id = factor(as.numeric(factor(id)))) %&gt;% lm(judgment ~ id - 1, data = .) %&gt;% tidy() %&gt;% mutate(p.value = scales::pvalue(p.value)) ## # A tibble: 33 × 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 id1 3.9 0.314 12.4 &lt;0.001 ## 2 id2 3.55 0.314 11.3 &lt;0.001 ## 3 id3 3.75 0.314 11.9 &lt;0.001 ## 4 id4 3.4 0.314 10.8 &lt;0.001 ## 5 id5 3.45 0.314 11.0 &lt;0.001 ## 6 id6 3.65 0.314 11.6 &lt;0.001 ## 7 id7 3.35 0.314 10.7 &lt;0.001 ## 8 id8 3.95 0.314 12.6 &lt;0.001 ## 9 id9 3.68 0.323 11.4 &lt;0.001 ## 10 id10 3.9 0.314 12.4 &lt;0.001 ## # … with 23 more rows Question: Why are neither of these models ideal? You can think of linear mixed models as the happy medium between these two approaches. While we will not talk about the underlying machinery, we will try to get an intuition about what is happening. 5.3 Random Effects Using lme4 (Bates et al. 2015), we will fit four models and visualize the outputs to see what the different random effects do. I will fit four different models below. The predictors (or fixed effects, as they are often called) are the same for all of them, in that the include main effects for at-issueness and trigger type, as well as their interaction. The only difference will be so-called random effects structure. This allows us to account for a number of systematic variance that is like not due to our experimental manipulations. Again, this includes by-participant variation (some may be tired or simply do not like our sentences), but a portion of it will also be indirectly related to our predictors. It is not hard to imagine that some speakers will react more or less strongly to an experimental manipulation than others. If you’ve ever asked more than one person for judgments on a minimal pair, you’ll know what I’m talking about. Note first that it is not possible to fit a linear mixed model without random effects (i.e., a model that looks similar to the ones we used with the lm() call, just with the call lmer()): # load the required package library(lme4) # fit the model (should fail) psp_lmm_id &lt;- lmer(judgment ~ issue * trigger_cat, data = d_psp, REML = FALSE) Error: No random effects terms specified in formula Let’s go through the four models that do include various specifications of the random effects step by step: the first one just has by-participant random intercepts. That is, the lines that the model estimates from the data are allowed the start at different heights. But the slopes must be the same between all of them. in the second one, we will have random intercepts and also random slopes for the trigger type manipulation. This is to account for the fact that some people may like hard or soft presupposition triggers better than others. in the third one, we have random intercepts and random intercepts for at-issueness. Again, some people might be more or less tolerant with this difference. In the fourth model, we have random intercepts and random slopes for both trigger type and at-issueness, this is done using the + to combine the two. in the final models, we additionally have random slopes for the interaction between the two predictors (as indicated by using the *. psp_lmm_id &lt;- lmer(judgment ~ issue * trigger_cat + (1 | id), data = d_psp, REML = FALSE) psp_lmm_trigger &lt;- lmer(judgment ~ issue * trigger_cat + (trigger_cat | id), data = d_psp, REML = FALSE) psp_lmm_issue &lt;- lmer(judgment ~ issue * trigger_cat + (issue | id), data = d_psp, REML = FALSE) psp_lmm_both &lt;- lmer(judgment ~ issue * trigger_cat + (issue + trigger_cat | id), data = d_psp, REML = FALSE) psp_lmm_int &lt;- lmer(judgment ~ issue * trigger_cat + (issue * trigger_cat | id), data = d_psp, REML = FALSE) ## boundary (singular) fit: see help(&#39;isSingular&#39;) For now, the results of the different models are unimportant. The main point here is to understand what the different random effects structures do with the data. What we want to know is thus this: How do the fitted lines differ between the models? Let’s write a function so that we do not have to repeat the code for the plots over and over again. (To make everything a little bit prettier, I’ll also change the colors) colfunc &lt;- colorRampPalette(c(colors[2], colors[1])) base_plot &lt;- function(fitted, title) { d_psp %&gt;% ggplot() + facet_wrap(~issue) + geom_line(data = d_psp, aes(x = trigger_cat_d, y = fitted, group = id, color = id), alpha = .5) + guides(color = &quot;none&quot;) + scale_color_manual(values = colfunc(length(unique(d_psp$id)))) + labs( x = &quot;Trigger Type&quot;, y = &quot;Judgments&quot;, title = title ) + scale_x_continuous(breaks = c(0, 1)) } Recall what our normal models look like: The lines should actually not be fully opaque, but the are all the same, so they overlap. What our standard (and in this case wrong) model does is to disregard the participant information. psp_lm &lt;- lm(judgment ~ issue * trigger_cat, data = d_psp) d_psp$standard_lm &lt;- predict(psp_lm) base_plot(d_psp$standard_lm, &quot;Standard Regression&quot;) Note that even though this model is wrong because it will wrongly estimate the standard errors (and thus our \\(p\\)-values will be off), the coefficients should all be correctly estimated. Now let’s turn to the mixed models. To add the fitted values to the data frame, we can use the predict() function: d_psp$model_id_fitted &lt;- predict(psp_lmm_id) d_psp$model_trigger_fitted &lt;- predict(psp_lmm_trigger) d_psp$model_issue_fitted &lt;- predict(psp_lmm_issue) d_psp$model_both_fitted &lt;- predict(psp_lmm_both) d_psp$model_int_fitted &lt;- predict(psp_lmm_int) head(d_psp) ## # A tibble: 6 × 12 ## id itemid trigger_cat issue judgment trigger_cat_d standard_lm model_id_fitted ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 auncqkpvl6b3rp0d3g833vmuh6 gewinnen2 soft at-issue 5 1 3.73 3.69 ## 2 auncqkpvl6b3rp0d3g833vmuh6 schaffen1 soft at-issue 1 1 3.73 3.69 ## 3 auncqkpvl6b3rp0d3g833vmuh6 auch3 hard at-issue 1 0 2.00 1.96 ## 4 auncqkpvl6b3rp0d3g833vmuh6 entdecken1 soft at-issue 5 1 3.73 3.69 ## 5 auncqkpvl6b3rp0d3g833vmuh6 schaffen3 soft at-issue 1 1 3.73 3.69 ## 6 auncqkpvl6b3rp0d3g833vmuh6 cleft3 hard non-at-issue 5 0 4.51 4.48 ## model_trigger_fitted model_issue_fitted model_both_fitted model_int_fitted ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3.82 3.48 3.65 3.73 ## 2 3.82 3.48 3.65 3.73 ## 3 1.88 1.75 1.51 1.39 ## 4 3.82 3.48 3.65 3.73 ## 5 3.82 3.48 3.65 3.73 ## 6 4.39 4.70 4.52 4.63 Now, let’s get to the plots. Below is the visualization for the random intercept (by participants) only model. As we expected, there is a line for each of the participants, because they are all slightly different from each other. This allows us to account for quite a bit of variance already that in our wrong model above would have looked like unsystematic noise. base_plot(d_psp$model_id_fitted, &quot;Random participant intercepts only&quot;) base_plot(d_psp$model_trigger_fitted, &quot;Random participant intercepts and Trigger random slopes&quot;) base_plot(d_psp$model_issue_fitted, &quot;Random participant intercepts and Issueness random slopes&quot;) base_plot(d_psp$model_both_fitted, &quot;Random participant intercepts and Trigger + Issueness random slopes&quot;) base_plot(d_psp$model_int_fitted, &quot;Random participant intercepts and Trigger * Issueness random slopes&quot;) 5.3.1 Let’s have a bit more of a detailed look To get a sense of what is going on, let’s look at a super simple model again: mod &lt;- lmer(judgment ~ issue + (1 | id), data = d_psp) Here’s the estimated model for each person. Notice that here the slopes are the same for everybody: coef(mod) ## $id ## (Intercept) issuenon-at-issue ## 2698jqo2b9fbpm7k69oo82c8j5 3.0318409 1.5751218 ## 3gkj6ts9s53vtfhk8v1r4d07r1 2.7952214 1.5751218 ## 44vjsg96djepi7rsqs9oken6m3 2.9304325 1.5751218 ## 4lvielgqtqtdssufsntp22ktr3 2.6938130 1.5751218 ## 54blvd1463nfcof4tainpg6q21 2.7276158 1.5751218 ## 6hk2ug38419qrom7vcr9jqui64 2.8628269 1.5751218 ## 7o47umnslij8c5ggm50k4ofg75 2.6600102 1.5751218 ## 8p2a9d7h7hm9031occ2uiuvpo7 3.0656437 1.5751218 ## 8v3ac1g6ciqibvqu16me22agn4 2.9131341 1.5751218 ## atf56fhfc894vb2cvl7el4ibc0 3.0318409 1.5751218 ## auncqkpvl6b3rp0d3g833vmuh6 2.8290242 1.5751218 ## c9cuhu3a777bt4j39kum6kp3v6 3.0758502 1.5751218 ## cni0v3n4u08lanbc41lvb91nl4 2.8966297 1.5751218 ## cr1c4eo2mcvntbj4d7e88o6u96 2.7614186 1.5751218 ## ebaji4itu78e23odap7indjt82 2.8290242 1.5751218 ## eegen1ic28r6uoh3kr4rkalil1 2.6938130 1.5751218 ## ftj7hf1ff73reu9t948dea7lc5 3.3698688 1.5751218 ## iha5atv6engbc4g28sriiki7h3 3.4374744 1.5751218 ## ikko382of1dkgpdjc5tuhicni0 3.0994465 1.5751218 ## lgob6021gj4v1qgeloa75ubo06 2.8966297 1.5751218 ## mkf458e7ms0o4fmfi9ueet0il0 2.6600102 1.5751218 ## ocbq4ok768sfqi7krh97qiekg3 3.4036716 1.5751218 ## ovsei19p9v0hlt1u2eva9mnl82 3.0994465 1.5751218 ## p87dj0rkahhluvbrgn5485bsl4 2.2205739 1.5751218 ## pgrg7dm45mkd6r00m6iec0nl62 2.7276158 1.5751218 ## q9hue6iu5q78a62r3l26563101 1.9839544 1.5751218 ## qia3n8stdcdbrgpq6pgrfhuhf4 3.1670520 1.5751218 ## rbe3eovtakfqc6bi6hbk7vdv15 2.7276158 1.5751218 ## svbr1nj2bfklg6fohe8ofvb3r5 2.6262074 1.5751218 ## tf1e0cg09vglistu7suqprqla3 2.9304325 1.5751218 ## tn5s5k5495gs8as4bsfkikq8v7 2.4233907 1.5751218 ## u51sl2l9h8i37g94h1i5hrolf1 2.7614186 1.5751218 ## vbhdv8atm0j8ojg4tr3brde127 3.1670520 1.5751218 ## ## attr(,&quot;class&quot;) ## [1] &quot;coef.mer&quot; Just the fixed effects: fixef(mod) ## (Intercept) issuenon-at-issue ## 2.8636364 1.5751218 Just the random effects, i.e., how much the intercept was shifted up or down for each person: ranef(mod) ## $id ## (Intercept) ## 2698jqo2b9fbpm7k69oo82c8j5 0.16820452644 ## 3gkj6ts9s53vtfhk8v1r4d07r1 -0.06841500311 ## 44vjsg96djepi7rsqs9oken6m3 0.06679615663 ## 4lvielgqtqtdssufsntp22ktr3 -0.16982337291 ## 54blvd1463nfcof4tainpg6q21 -0.13602058298 ## 6hk2ug38419qrom7vcr9jqui64 -0.00080942324 ## 7o47umnslij8c5ggm50k4ofg75 -0.20362616285 ## 8p2a9d7h7hm9031occ2uiuvpo7 0.20200731637 ## 8v3ac1g6ciqibvqu16me22agn4 0.04949778043 ## atf56fhfc894vb2cvl7el4ibc0 0.16820452644 ## auncqkpvl6b3rp0d3g833vmuh6 -0.03461221317 ## c9cuhu3a777bt4j39kum6kp3v6 0.21221386946 ## cni0v3n4u08lanbc41lvb91nl4 0.03299336670 ## cr1c4eo2mcvntbj4d7e88o6u96 -0.10221779304 ## ebaji4itu78e23odap7indjt82 -0.03461221317 ## eegen1ic28r6uoh3kr4rkalil1 -0.16982337291 ## ftj7hf1ff73reu9t948dea7lc5 0.50623242579 ## iha5atv6engbc4g28sriiki7h3 0.57383800566 ## ikko382of1dkgpdjc5tuhicni0 0.23581010631 ## lgob6021gj4v1qgeloa75ubo06 0.03299336670 ## mkf458e7ms0o4fmfi9ueet0il0 -0.20362616285 ## ocbq4ok768sfqi7krh97qiekg3 0.54003521573 ## ovsei19p9v0hlt1u2eva9mnl82 0.23581010631 ## p87dj0rkahhluvbrgn5485bsl4 -0.64306243201 ## pgrg7dm45mkd6r00m6iec0nl62 -0.13602058298 ## q9hue6iu5q78a62r3l26563101 -0.87968196155 ## qia3n8stdcdbrgpq6pgrfhuhf4 0.30341568618 ## rbe3eovtakfqc6bi6hbk7vdv15 -0.13602058298 ## svbr1nj2bfklg6fohe8ofvb3r5 -0.23742895278 ## tf1e0cg09vglistu7suqprqla3 0.06679615663 ## tn5s5k5495gs8as4bsfkikq8v7 -0.44024569239 ## u51sl2l9h8i37g94h1i5hrolf1 -0.10221779304 ## vbhdv8atm0j8ojg4tr3brde127 0.30341568618 ## ## with conditional variances for &quot;id&quot; 5.3.2 Model output comparisons summary(psp_lmm_id) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ issue * trigger_cat + (1 | id) ## Data: d_psp ## ## AIC BIC logLik deviance df.resid ## 1874.6 1901.6 -931.3 1862.6 651 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.57054 -0.55493 0.14589 0.62245 3.15696 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.15236 0.39034 ## Residual 0.92704 0.96283 ## Number of obs: 657, groups: id, 33 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.00000 0.10117 92.85760 19.768 &lt; 0.00000000000000022 *** ## issuenon-at-issue 2.51579 0.10653 624.29187 23.617 &lt; 0.00000000000000022 *** ## trigger_catsoft 1.72727 0.10600 624.04609 16.294 &lt; 0.00000000000000022 *** ## issuenon-at-issue:trigger_catsoft -1.87942 0.15028 624.16968 -12.506 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) issn-- trggr_ ## issunn-t-ss -0.521 ## trggr_ctsft -0.524 0.498 ## issnn-t-s:_ 0.370 -0.709 -0.705 cat(&quot;####\\n\\n&quot;) ## #### summary(psp_lmm_trigger) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ issue * trigger_cat + (trigger_cat | id) ## Data: d_psp ## ## AIC BIC logLik deviance df.resid ## 1858.7 1894.6 -921.3 1842.7 649 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.59485 -0.52061 0.07376 0.60074 3.17697 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.12616 0.35519 ## trigger_catsoft 0.24922 0.49922 -0.185 ## Residual 0.86123 0.92803 ## Number of obs: 657, groups: id, 33 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.000000 0.095093 64.333533 21.032 &lt; 0.00000000000000022 *** ## issuenon-at-issue 2.515429 0.102690 592.123435 24.495 &lt; 0.00000000000000022 *** ## trigger_catsoft 1.727273 0.134131 64.858071 12.877 &lt; 0.00000000000000022 *** ## issuenon-at-issue:trigger_catsoft -1.879065 0.144860 591.648471 -12.972 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) issn-- trggr_ ## issunn-t-ss -0.535 ## trggr_ctsft -0.487 0.379 ## issnn-t-s:_ 0.379 -0.709 -0.537 cat(&quot;####\\n\\n&quot;) ## #### summary(psp_lmm_issue) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ issue * trigger_cat + (issue | id) ## Data: d_psp ## ## AIC BIC logLik deviance df.resid ## 1875.3 1911.2 -929.6 1859.3 649 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.52991 -0.56302 0.13638 0.55042 3.20614 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.173402 0.41642 ## issuenon-at-issue 0.097873 0.31285 -0.339 ## Residual 0.901238 0.94934 ## Number of obs: 657, groups: id, 33 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.00000 0.10352 59.04491 19.320 &lt; 0.00000000000000022 *** ## issuenon-at-issue 2.51616 0.11833 88.69901 21.264 &lt; 0.00000000000000022 *** ## trigger_catsoft 1.72727 0.10452 591.18494 16.526 &lt; 0.00000000000000022 *** ## issuenon-at-issue:trigger_catsoft -1.87980 0.14819 591.64714 -12.685 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) issn-- trggr_ ## issunn-t-ss -0.555 ## trggr_ctsft -0.505 0.442 ## issnn-t-s:_ 0.356 -0.629 -0.705 cat(&quot;####\\n\\n&quot;) ## #### summary(psp_lmm_both) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ issue * trigger_cat + (issue + trigger_cat | id) ## Data: d_psp ## ## AIC BIC logLik deviance df.resid ## 1856.7 1906.1 -917.4 1834.7 646 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.68352 -0.56292 0.12466 0.54356 3.21611 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.20226 0.44973 ## issuenon-at-issue 0.11195 0.33459 -0.672 ## trigger_catsoft 0.25540 0.50537 -0.377 0.588 ## Residual 0.82998 0.91103 ## Number of obs: 657, groups: id, 33 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.00000 0.10564 41.86307 18.933 &lt; 0.00000000000000022 *** ## issuenon-at-issue 2.51532 0.11642 83.47478 21.605 &lt; 0.00000000000000022 *** ## trigger_catsoft 1.72727 0.13342 63.51753 12.947 &lt; 0.00000000000000022 *** ## issuenon-at-issue:trigger_catsoft -1.87895 0.14220 559.23378 -13.213 &lt; 0.00000000000000022 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) issn-- trggr_ ## issunn-t-ss -0.658 ## trggr_ctsft -0.541 0.518 ## issnn-t-s:_ 0.335 -0.614 -0.530 cat(&quot;####\\n\\n&quot;) ## #### summary(psp_lmm_int) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ issue * trigger_cat + (issue * trigger_cat | id) ## Data: d_psp ## ## AIC BIC logLik deviance df.resid ## 1851.5 1918.8 -910.8 1821.5 642 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.83343 -0.49357 0.14374 0.56197 3.33018 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.32132 0.56685 ## issuenon-at-issue 0.29742 0.54537 -0.837 ## trigger_catsoft 0.67379 0.82085 -0.657 0.858 ## issuenon-at-issue:trigger_catsoft 0.40649 0.63757 0.759 -0.802 -0.959 ## Residual 0.79588 0.89212 ## Number of obs: 657, groups: id, 33 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.00000 0.12067 33.03049 16.5746 &lt; 0.00000000000000022 *** ## issuenon-at-issue 2.51525 0.13694 37.00472 18.3669 &lt; 0.00000000000000022 *** ## trigger_catsoft 1.72727 0.17339 33.03066 9.9617 0.0000000000176434 *** ## issuenon-at-issue:trigger_catsoft -1.87888 0.17806 40.51940 -10.5519 0.0000000000003424 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) issn-- trggr_ ## issunn-t-ss -0.766 ## trggr_ctsft -0.673 0.693 ## issnn-t-s:_ 0.611 -0.746 -0.805 ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) 5.4 What do I do about my \\(p\\)-values? There is quite a bit of controversy about how to get p-values from linear mixed models. The approach that I will present here is the (as far as I know) most consersative one. This works as follows: We compare several models of different complexity and see whether the more restrictive model (i.e., the one with one less estimation to do) and the one that includes our factor of interest differ from each other with respect to how much variance they explain. If there is one such difference, we can be pretty sure that that factor is significant. If not, then there is no effect. Let’s go through this step by step. First, we need all the models. In terms of random effects structure, let’s use the most complex one: And also, let’s finally use all of the data: d_psp &lt;- read_csv(here(&quot;assets&quot;, &quot;data&quot;, &quot;psp-data.csv&quot;)) head(d_psp) ## # A tibble: 6 × 16 ## id gender age months age_months_dec stage median_group item itemid ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children schaffen4_at schaffen4 ## 2 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children schaffen1_non schaffen1 ## 3 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children cleft2_non cleft2 ## 4 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children appRel3_non appRel3 ## 5 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children appRel9_non appRel9 ## 6 3536fed0235c7b34a33ddf06fb91b390 w 5 1 5.1 Children Older Children gewinnen2_non gewinnen2 ## trigger trigger_cat issue judgment question_type presentation_order presentation_cat ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 schaffen soft at-issue 5 wh-Question 1 1st half ## 2 schaffen soft non-at-issue 5 wh-Question 2 1st half ## 3 cleft hard non-at-issue 5 wh-Question 3 1st half ## 4 appRel appo non-at-issue 5 wh-Question 4 1st half ## 5 appRel appo non-at-issue 4 wh-Question 5 1st half ## 6 gewinnen soft non-at-issue 4 Polar Question 6 1st half psp_0 &lt;- lmer( judgment ~ 1 + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, REML = FALSE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;) ) psp_1a &lt;- lmer( judgment ~ issue + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, REML = FALSE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;) ) psp_1b &lt;- lmer( judgment ~ trigger_cat + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, REML = FALSE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;) ) psp_1c &lt;- lmer( judgment ~ stage + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, REML = FALSE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;) ) And now let’s compare them. To do this, we use the anova() command: anova(psp_0, psp_1a) ## Data: d_psp ## Models: ## psp_0: judgment ~ 1 + (issue + trigger_cat | id) + (issue | itemid) ## psp_1a: judgment ~ issue + (issue + trigger_cat | id) + (issue | itemid) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## psp_0 15 5162.58 5243.81 -2566.29 5132.58 ## psp_1a 16 5128.91 5215.56 -2548.45 5096.91 35.6719 1 0.0000000023351 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(psp_0, psp_1b) ## Data: d_psp ## Models: ## psp_0: judgment ~ 1 + (issue + trigger_cat | id) + (issue | itemid) ## psp_1b: judgment ~ trigger_cat + (issue + trigger_cat | id) + (issue | itemid) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## psp_0 15 5162.58 5243.81 -2566.29 5132.58 ## psp_1b 17 5158.68 5250.75 -2562.34 5124.68 7.89444 2 0.019308 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 anova(psp_0, psp_1c) ## Data: d_psp ## Models: ## psp_0: judgment ~ 1 + (issue + trigger_cat | id) + (issue | itemid) ## psp_1c: judgment ~ stage + (issue + trigger_cat | id) + (issue | itemid) ## npar AIC BIC logLik deviance Chisq Df Pr(&gt;Chisq) ## psp_0 15 5162.58 5243.81 -2566.29 5132.58 ## psp_1c 16 5163.66 5250.31 -2565.83 5131.66 0.92021 1 0.33742 But that’s a lot of work, I hear you say. Is there a better way? Yes, the afex has a function that will do all of the work for you: library(afex) psp_lmm &lt;- mixed( judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, method = &quot;LRT&quot;, expand_re = TRUE, REML = FALSE, check_contrasts = TRUE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;) ) ## Contrasts set to contr.sum for the following variables: stage, issue, trigger_cat, id, itemid summary(psp_lmm) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ stage * issue * trigger_cat + (1 + re1.issue1 + re1.trigger_cat1 + ## re1.trigger_cat2 | id) + (1 + re2.issue1 | itemid) ## Data: data ## Control: lmerControl(optCtrl = list(maxfun = 1000000), optimizer = &quot;bobyqa&quot;) ## ## AIC BIC logLik deviance df.resid ## 5001.9 5142.7 -2474.9 4949.9 1636 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.32629 -0.53953 0.07995 0.60519 3.07369 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.2877350 0.536409 ## re1.issue1 0.0265034 0.162799 0.343 ## re1.trigger_cat1 0.0241334 0.155349 -0.081 -0.139 ## re1.trigger_cat2 0.0314903 0.177455 -0.145 0.669 0.396 ## itemid (Intercept) 0.0273183 0.165282 ## re2.issue1 0.0098093 0.099042 0.304 ## Residual 0.9957573 0.997876 ## Number of obs: 1662, groups: id, 56; itemid, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.501566 0.082965 66.375092 42.2052 &lt; 0.00000000000000022 *** ## stage1 -0.024771 0.077262 56.540660 -0.3206 0.7496905 ## issue1 -0.539905 0.038623 39.454706 -13.9787 &lt; 0.00000000000000022 *** ## trigger_cat1 -0.236224 0.059691 33.443798 -3.9575 0.0003733 *** ## trigger_cat2 -0.117529 0.061341 35.248683 -1.9160 0.0635043 . ## stage1:issue1 -0.337886 0.034321 59.862353 -9.8449 0.00000000000003945 *** ## stage1:trigger_cat1 -0.126910 0.041823 57.809204 -3.0345 0.0036072 ** ## stage1:trigger_cat2 -0.087479 0.043458 58.251925 -2.0129 0.0487513 * ## issue1:trigger_cat1 -0.111251 0.044405 30.756521 -2.5054 0.0177418 * ## issue1:trigger_cat2 -0.232307 0.044934 29.937929 -5.1700 0.00001449595699166 *** ## stage1:issue1:trigger_cat1 -0.091389 0.036723 1310.246456 -2.4886 0.0129475 * ## stage1:issue1:trigger_cat2 -0.126099 0.036727 1326.319222 -3.4334 0.0006144 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) stage1 issue1 trgg_1 trgg_2 stg1:1 st1:_1 st1:_2 is1:_1 is1:_2 s1:1:_1 ## stage1 -0.171 ## issue1 0.232 -0.037 ## trigger_ct1 -0.030 0.006 -0.035 ## trigger_ct2 -0.042 0.009 0.164 -0.378 ## stage1:iss1 -0.038 0.213 -0.192 0.005 -0.030 ## stg1:trgg_1 0.008 -0.040 0.009 -0.144 0.040 -0.044 ## stg1:trgg_2 0.012 -0.076 -0.041 0.039 -0.147 0.237 -0.245 ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] anova(psp_lmm) ## Mixed Model Anova Table (Type 3 tests, LRT-method) ## ## Model: judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | ## Model: id) + (issue | itemid) ## Data: d_psp ## Df full model: 26 ## Df Chisq Chi Df Pr(&gt;Chisq) ## stage 25 0.1026 1 0.74872743 ## issue 25 69.0498 1 &lt; 0.000000000000000222 *** ## trigger_cat 24 22.3172 2 0.000014252140351343 *** ## stage:issue 25 57.6565 1 0.000000000000031213 *** ## stage:trigger_cat 24 15.1527 2 0.00051244 *** ## issue:trigger_cat 24 33.5860 2 0.000000050920357821 *** ## stage:issue:trigger_cat 24 34.5869 2 0.000000030870781952 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 References "],["6-loose-ends.html", "6 Loose ends 6.1 Long computation times 6.2 Convergence Issues 6.3 Investigating interactions and pairwise comparisons 6.4 Not recommended: all pairwise comparisons 6.5 More emmeans: Interactions 6.6 Ordinal data 6.7 Reporting 6.8 Open questions 6.9 Fin!", " 6 Loose ends 6.1 Long computation times With complex LMMs and with the likelihood ratio method for obtaining \\(p\\)-values, it often takes quite a while to get your results. It simply takes time to fit all of the models. However, this is a good situation for using parallel processing. With this approach, your computer, which likely has several cores in the CPU, will use all of the available cores, instead of just using 1. For example, my computer I am preparing this script on has 4 cores. Ideally, using parallel processing you might get your results four times as fast. Just remember to stop the cluster after you’re done. library(parallel) # enable parallel processing (nc &lt;- detectCores()) ## [1] 4 cl &lt;- makeCluster(rep(&quot;localhost&quot;, nc)) afex_lmm &lt;- mixed( judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | id) + (issue | itemid), data = d_psp, method = &quot;LRT&quot;, expand_re = TRUE, REML = FALSE, check_contrasts = TRUE, control = lmerControl(optCtrl = list(maxfun = 1e6), optimizer = &quot;bobyqa&quot;), cl = cl ) ## Contrasts set to contr.sum for the following variables: stage, issue, trigger_cat, id, itemid stopCluster(cl) 6.2 Convergence Issues It is unfortunately quite common to run into convergence issues, which means that that lm4e, which afex::mixed() calls under the hood, could not estimate the the values from your data. Here’s an example (where I just use a portion of the data): set.seed(1111) # sample a fourth of the data d_psp_short &lt;- sample_n(d_psp, length(d_psp$id) / 4) # does not converge and is singular afex_lmm_fails &lt;- lmer( judgment ~ stage * issue * trigger_cat + (issue * trigger_cat | id) + (issue + stage | itemid), data = d_psp_short, ) ## boundary (singular) fit: see help(&#39;isSingular&#39;) ## Warning: Model failed to converge with 1 negative eigenvalue: -6.9e-03 Sometimes, the only way out of this is to increase the amount of observations. Luckily, sometimes other approaches work. 6.2.1 Optimizers To estimate the random intercepts and slopes, an optimizer function is used. afex lists the following: ## [1] &quot;bobyqa&quot; &quot;Nelder_Mead&quot; &quot;optimx&quot; &quot;nloptwrap&quot; &quot;nmkbw&quot; Sometimes, a different optimizer can help with convergence issues, but it is quite cumbersome to go through all of them together. afex includes the all_fit option for this reason (as well as a separate function, which I ignore here). Let’s see what happens here: # needs to be loaded explicitly to give us access to two more optimizing algorithms library(optimx) psp_lm &lt;- lmer( judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | id) + (issue | itemid), data = d_psp ) psp_lm_all &lt;- allFit( psp_lm, maxfun = 1e6, parallel = &quot;multicore&quot;, ncpus = detectCores() ) ## Loading required namespace: dfoptim And here we can check for convergence. You are looking for optimizers where it says “NULL”. These are the ones without issues. psp_lmm_all_ok &lt;- psp_lm_all[sapply(psp_lm_all, is, &quot;merMod&quot;)] lapply(psp_lmm_all_ok, function(x) x@optinfo$conv$lme4$messages) ## $bobyqa ## NULL ## ## $Nelder_Mead ## [1] &quot;unable to evaluate scaled gradient&quot; ## [2] &quot;Model failed to converge: degenerate Hessian with 1 negative eigenvalues&quot; ## ## $nlminbwrap ## NULL ## ## $`optimx.L-BFGS-B` ## [1] &quot;unable to evaluate scaled gradient&quot; ## [2] &quot;Model failed to converge: degenerate Hessian with 1 negative eigenvalues&quot; ## ## $nloptwrap.NLOPT_LN_NELDERMEAD ## NULL ## ## $nloptwrap.NLOPT_LN_BOBYQA ## NULL For more details on this approach, you can have a look at the lovely tutorial here: https://joshua-nugent.github.io/allFit/ 6.2.2 Reducing the random effects structure One other option is to reduce the random effects structure. As you might have noticed, I could have fit a more complex model given the way that my data are set up. For example, I could have included by-participant random slopes that included the interaction between at-issueness and trigger type. In addition, I could have included the age group as a random slope for the items plus the interaction with at-issueness. Let’s see how this goes: # re-enable the cluster nc &lt;- detectCores() cl &lt;- makeCluster(rep(&quot;localhost&quot;, nc)) psp_lmm_all_more &lt;- mixed( judgment ~ stage * issue * trigger_cat + (issue * trigger_cat | id) + (stage * issue | itemid), data = d_psp, method = &quot;LRT&quot;, expand_re = TRUE, REML = FALSE, check_contrasts = TRUE, all_fit = TRUE, cl = cl ) ## Contrasts set to contr.sum for the following variables: stage, issue, trigger_cat, id, itemid stopCluster(cl) summary(psp_lmm_all_more) ## Linear mixed model fit by maximum likelihood . t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ stage * issue * trigger_cat + (1 + re1.issue1 + re1.trigger_cat1 + ## re1.trigger_cat2 + re1.issue1_by_trigger_cat1 + re1.issue1_by_trigger_cat2 | ## id) + (1 + re2.stage1 + re2.issue1 + re2.stage1_by_issue1 | itemid) ## Data: data ## Control: ctrl ## ## AIC BIC logLik deviance df.resid ## 5014.2 5252.4 -2463.1 4926.2 1618 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.54791 -0.52090 0.09529 0.58183 3.06576 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.2892336 0.537804 ## re1.issue1 0.0283392 0.168342 0.328 ## re1.trigger_cat1 0.0311125 0.176387 -0.073 -0.075 ## re1.trigger_cat2 0.0384941 0.196199 -0.133 0.564 0.189 ## re1.issue1_by_trigger_cat1 0.0129534 0.113813 0.072 0.257 0.675 -0.186 ## re1.issue1_by_trigger_cat2 0.0219820 0.148263 0.233 0.453 0.129 0.891 -0.352 ## itemid (Intercept) 0.0273469 0.165369 ## re2.stage1 0.0019542 0.044206 0.003 ## [ reached getOption(&quot;max.print&quot;) -- omitted 3 rows ] ## Number of obs: 1662, groups: id, 56; itemid, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 3.503822 0.083490 67.430158 41.9671 &lt; 0.00000000000000022 *** ## stage1 -0.025757 0.078207 58.324547 -0.3293 0.7430731 ## issue1 -0.541921 0.041008 39.608389 -13.2151 0.00000000000000042 *** ## trigger_cat1 -0.238799 0.061440 36.046547 -3.8867 0.0004185 *** ## trigger_cat2 -0.111185 0.062935 37.828248 -1.7667 0.0853518 . ## stage1:issue1 -0.335417 0.037886 48.658274 -8.8533 0.00000000001037412 *** ## stage1:trigger_cat1 -0.125717 0.045644 50.739733 -2.7543 0.0081446 ** ## stage1:trigger_cat2 -0.092144 0.047017 53.432857 -1.9598 0.0552428 . ## issue1:trigger_cat1 -0.109553 0.050163 29.788884 -2.1839 0.0369781 * ## issue1:trigger_cat2 -0.235881 0.052398 32.017009 -4.5017 0.00008392491146185 *** ## stage1:issue1:trigger_cat1 -0.093858 0.045048 41.377588 -2.0835 0.0434234 * ## stage1:issue1:trigger_cat2 -0.121355 0.047083 43.329337 -2.5775 0.0134367 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) stage1 issue1 trgg_1 trgg_2 stg1:1 st1:_1 st1:_2 is1:_1 is1:_2 s1:1:_1 ## stage1 -0.180 ## issue1 0.180 0.030 ## trigger_ct1 -0.028 0.005 -0.022 ## trigger_ct2 -0.045 0.012 0.146 -0.386 ## stage1:iss1 0.052 0.138 -0.344 0.002 -0.025 ## stg1:trgg_1 0.006 -0.035 0.004 -0.178 0.056 -0.022 ## stg1:trgg_2 0.016 -0.075 -0.035 0.056 -0.174 0.193 -0.288 ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] ## optimizer (bobyqa) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) If you look at the very bottom of the summary, you will see that the model, while it did converge, is singular. You can get a little bit more detail about this generally by using ?isSingular. Here’s another example with the smaller data set: # converges, but singular afex_lmm_failsnomore &lt;- lmer( judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | id) + (issue | itemid), data = d_psp_short ) ## boundary (singular) fit: see help(&#39;isSingular&#39;) summary(afex_lmm_failsnomore) # look at all the correlations that are |1| for the random effects! ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | id) + (issue | itemid) ## Data: d_psp_short ## ## REML criterion at convergence: 1266.7 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.17805 -0.61351 0.06361 0.62023 2.46714 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## id (Intercept) 0.215371791 0.4640817 ## issuenon-at-issue 0.000021259 0.0046107 -1.000 ## trigger_cathard 0.004377732 0.0661644 1.000 -1.000 ## trigger_catsoft 0.019594910 0.1399818 1.000 -1.000 1.000 ## itemid (Intercept) 0.145338001 0.3812322 ## issuenon-at-issue 0.071541585 0.2674726 -1.000 ## Residual 1.012501261 1.0062312 ## Number of obs: 415, groups: id, 56; itemid, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.06151 0.21113 43.86189 9.7642 0.0000000000014346 *** ## stageChildren 0.90548 0.29775 115.77287 3.0411 0.0029162 ** ## issuenon-at-issue 2.12111 0.24680 99.43875 8.5945 0.0000000000001221 *** ## trigger_cathard 0.12879 0.29207 42.30048 0.4409 0.6614995 ## trigger_catsoft 1.45050 0.29225 45.25846 4.9632 0.0000102757442677 *** ## stageChildren:issuenon-at-issue -1.36049 0.37830 301.40893 -3.5964 0.0003769 *** ## stageChildren:trigger_cathard 0.38819 0.37440 315.17880 1.0368 0.3006034 ## stageChildren:trigger_catsoft -0.57392 0.40621 304.29366 -1.4129 0.1587130 ## issuenon-at-issue:trigger_cathard 0.30971 0.35831 99.74995 0.8644 0.3894596 ## issuenon-at-issue:trigger_catsoft -1.23615 0.35565 100.76895 -3.4758 0.0007528 *** ## stageChildren:issuenon-at-issue:trigger_cathard -0.99467 0.52659 326.10431 -1.8889 0.0597950 . ## stageChildren:issuenon-at-issue:trigger_catsoft 0.53582 0.53992 345.53739 0.9924 0.3216974 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) stgChl issn-- trggr_cth trggr_cts stC:-- stgChldrn:trggr_cth stgChldrn:trggr_cts ## stageChldrn -0.488 ## issunn-t-ss -0.652 0.337 ## trggr_cthrd -0.604 0.269 0.470 ## trggr_ctsft -0.591 0.256 0.467 0.453 ## stgChldr:-- 0.306 -0.668 -0.588 -0.221 -0.215 ## stgChldrn:trggr_cth 0.296 -0.636 -0.268 -0.503 -0.226 0.530 ## stgChldrn:trggr_cts 0.261 -0.555 -0.241 -0.208 -0.469 0.478 0.480 ## issnn-t-ss:trggr_cth issnn-t-ss:trggr_cts stgChldrn:ssnn-t-ss:trggr_cth ## stageChldrn ## issunn-t-ss ## trggr_cthrd ## trggr_ctsft ## stgChldr:-- ## stgChldrn:trggr_cth ## stgChldrn:trggr_cts ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] ## optimizer (nloptwrap) convergence code: 0 (OK) ## boundary (singular) fit: see help(&#39;isSingular&#39;) While this is not a convergence issue, it is one that will often come up. The general take-away from this is the same (with some qualifications): if it can be avoided, you should reduce the random effects structure. Again, with the smaller data set, this approach works: afex_lmm_works &lt;- lmer( judgment ~ stage * issue * trigger_cat + (1 | id) + (1 | itemid), data = d_psp_short ) summary(afex_lmm_works) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [&#39;lmerModLmerTest&#39;] ## Formula: judgment ~ stage * issue * trigger_cat + (1 | id) + (1 | itemid) ## Data: d_psp_short ## ## REML criterion at convergence: 1270.9 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.080273 -0.531239 0.058106 0.642952 2.457750 ## ## Random effects: ## Groups Name Variance Std.Dev. ## id (Intercept) 0.27846 0.52769 ## itemid (Intercept) 0.06076 0.24650 ## Residual 1.03276 1.01625 ## Number of obs: 415, groups: id, 56; itemid, 30 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 2.05319 0.19620 117.89195 10.4649 &lt; 0.00000000000000022 *** ## stageChildren 0.87941 0.30087 260.35883 2.9229 0.0037729 ** ## issuenon-at-issue 2.11644 0.23693 365.85855 8.9326 &lt; 0.00000000000000022 *** ## trigger_cathard 0.10036 0.26016 113.37339 0.3858 0.7003899 ## trigger_catsoft 1.44762 0.26008 117.66123 5.5660 0.0000001666 *** ## stageChildren:issuenon-at-issue -1.31689 0.38629 323.00333 -3.4091 0.0007343 *** ## stageChildren:trigger_cathard 0.42128 0.36773 374.28221 1.1456 0.2526873 ## stageChildren:trigger_catsoft -0.52961 0.39808 370.01180 -1.3304 0.1841979 ## issuenon-at-issue:trigger_cathard 0.34551 0.34100 366.23042 1.0132 0.3116124 ## issuenon-at-issue:trigger_catsoft -1.22104 0.33702 362.15312 -3.6230 0.0003328 *** ## stageChildren:issuenon-at-issue:trigger_cathard -0.99948 0.53637 342.75470 -1.8634 0.0632594 . ## stageChildren:issuenon-at-issue:trigger_catsoft 0.46360 0.54645 359.20912 0.8484 0.3967970 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) stgChl issn-- trggr_cth trggr_cts stC:-- stgChldrn:trggr_cth stgChldrn:trggr_cts ## stageChldrn -0.554 ## issunn-t-ss -0.521 0.352 ## trggr_cthrd -0.592 0.312 0.392 ## trggr_ctsft -0.597 0.313 0.390 0.450 ## stgChldr:-- 0.324 -0.632 -0.631 -0.245 -0.240 ## stgChldrn:trggr_cth 0.339 -0.637 -0.288 -0.577 -0.258 0.516 ## stgChldrn:trggr_cts 0.315 -0.579 -0.262 -0.238 -0.533 0.469 0.476 ## issnn-t-ss:trggr_cth issnn-t-ss:trggr_cts stgChldrn:ssnn-t-ss:trggr_cth ## stageChldrn ## issunn-t-ss ## trggr_cthrd ## trggr_ctsft ## stgChldr:-- ## stgChldrn:trggr_cth ## stgChldrn:trggr_cts ## [ reached getOption(&quot;max.print&quot;) -- omitted 4 rows ] 6.3 Investigating interactions and pairwise comparisons Often, it is ill-advised to just stop after you have the significance data from your linear mixed model. This is often true in the presence of interactions, where the interpretation of the main effects gets much harder. In the case at hand, for example, the main effect for at-issueness is driven by the difference between hard and soft triggers in the at-issue condition. Of course, this can be inferred visually in a plot, but it is good to confirm these suspicions with follow-up tests of the statistical type as well. Another circumstance where follow-up analyses are important concerns post hoc analysis e.g., for predictors with more than two levels. To refresh your memory, here is the result of the LMM: anova(psp_lmm) ## Mixed Model Anova Table (Type 3 tests, LRT-method) ## ## Model: judgment ~ stage * issue * trigger_cat + (issue + trigger_cat | ## Model: id) + (issue | itemid) ## Data: d_psp ## Df full model: 26 ## Df Chisq Chi Df Pr(&gt;Chisq) ## stage 25 0.1026 1 0.74872743 ## issue 25 69.0498 1 &lt; 0.000000000000000222 *** ## trigger_cat 24 22.3172 2 0.000014252140351343 *** ## stage:issue 25 57.6565 1 0.000000000000031213 *** ## stage:trigger_cat 24 15.1527 2 0.00051244 *** ## issue:trigger_cat 24 33.5860 2 0.000000050920357821 *** ## stage:issue:trigger_cat 24 34.5869 2 0.000000030870781952 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Remember that the factor Trigger Type has three levels: soft triggers, hard triggers, and non-restrictive relative clauses. Question: Why is this a problem again? To remedy this, we can make use of so-called estimated marginal means and run follow-up tests. All of the necessary tools can be found in the emmeans package (Lenth 2019): library(emmeans) Other people do this differently, and simply run a series of \\(t\\)-tests on their original data and correct for multiple testing. We will not pursue this direction here, however. There are several (I believe good) reasons why: If you run \\(t\\)-tests on only a portion of the data, you lose statistical power. Why? Because we reduce the number of observations that we take into account. We already computed a model that took into account all the data and which estimated the variances using a random effects structure we thought matched our data. Why would we want to downgrade from this by using a \\(t\\)-test? LMMs are pretty good when the design is unbalanced or when the data are nested. The \\(t\\)-test does not have these advantages. So, what will the estimated marginal means approach do for us? In effect, we will use predicted values from our linear mixed model (rather than the raw data) and compute or follow-up analyses on those. These comparisons will also be \\(t\\)-tests, but since were using the estimates from our LMM, we get the best of both worlds. First, we need to create an object from our LMM that emmeans can deal will. To do this, we use the following code: # post hoc tests (ref_id &lt;- emmeans( psp_lmm, specs = c(&quot;stage&quot;, &quot;trigger_cat&quot;, &quot;issue&quot;), lmer.df = &quot;satterthwaite&quot; )) ## stage trigger_cat issue emmean SE df lower.CL upper.CL ## Adults appo at-issue 2.03 0.149 72.6 1.74 2.33 ## Children appo at-issue 3.20 0.176 79.0 2.84 3.55 ## Adults hard at-issue 2.04 0.155 71.0 1.73 2.34 ## Children hard at-issue 3.19 0.182 77.7 2.82 3.55 ## Adults soft at-issue 3.73 0.157 73.0 3.41 4.04 ## Children soft at-issue 3.59 0.187 78.4 3.22 3.96 ## Adults appo non-at-issue 4.19 0.130 70.4 3.93 4.45 ## Children appo non-at-issue 3.64 0.155 75.7 3.33 3.95 ## Adults hard non-at-issue 4.51 0.125 70.3 4.26 4.76 ## Children hard non-at-issue 3.80 0.148 76.5 3.51 4.10 ## [ reached getOption(&quot;max.print&quot;) -- omitted 2 rows ] ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 What we see above are the estimated values for the all of the conditions, including the estimated variance and the degrees of freedom. Then, we need to type in which comparisons interest us. In this case, I compare the non-restrictive relative clauses and the hard triggers for the adults. We do this by sum-coding the comparisons (all the effects with the 0 are ignored): # at-issue: NRRC v hard adults hard_appo_at_a &lt;- c(-1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0) And then we can run the \\(t\\)-tests on this contrast (using afex once more). Below I also added a contrast for non-restrictive relative clauses and hard triggers for the kids for illustrative purposes: hard_appo_at_c &lt;- c(0, -1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0) contrasts_lmm &lt;- contrast(ref_id, list( hard_appo_at_a = hard_appo_at_a, hard_appo_at_c = hard_appo_at_c ), adjust = &quot;holm&quot; ) summary(contrasts_lmm) ## contrast estimate SE df t.ratio p.value ## hard_appo_at_a 0.00236 0.152 45.3 0.016 1.0000 ## hard_appo_at_c -0.00708 0.176 64.0 -0.040 1.0000 ## ## Degrees-of-freedom method: satterthwaite ## P value adjustment: holm method for 2 tests confint(contrasts_lmm) ## contrast estimate SE df lower.CL upper.CL ## hard_appo_at_a 0.00236 0.152 45.3 -0.351 0.355 ## hard_appo_at_c -0.00708 0.176 64.0 -0.411 0.397 ## ## Degrees-of-freedom method: satterthwaite ## Confidence level used: 0.95 ## Conf-level adjustment: bonferroni method for 2 estimates 6.4 Not recommended: all pairwise comparisons Above we only looked at specific contrasts, but, you might say now, why don’t we just look at all of the comparisons? After all, we are correcting for multiple tests (using adjust = \"holm\"). I recommend against testing all possible pairwise comparisons, but I include how to do it here for illustrative purposes: pairs(ref_id, adjust = &quot;holm&quot;) ## contrast estimate SE df t.ratio p.value ## (Adults appo at-issue) - (Children appo at-issue) -1.16191 0.210 75.2 -5.533 &lt;.0001 ## (Adults appo at-issue) - (Adults hard at-issue) -0.00236 0.152 45.3 -0.016 1.0000 ## (Adults appo at-issue) - (Children hard at-issue) -1.15483 0.236 87.0 -4.900 0.0002 ## (Adults appo at-issue) - (Adults soft at-issue) -1.69496 0.164 58.6 -10.360 &lt;.0001 ## (Adults appo at-issue) - (Children soft at-issue) -1.55652 0.239 94.3 -6.512 &lt;.0001 ## (Adults appo at-issue) - (Adults appo non-at-issue) -2.16086 0.139 68.1 -15.560 &lt;.0001 ## (Adults appo at-issue) - (Children appo non-at-issue) -1.60567 0.207 88.2 -7.772 &lt;.0001 ## (Adults appo at-issue) - (Adults hard non-at-issue) -2.47475 0.145 67.4 -17.017 &lt;.0001 ## (Adults appo at-issue) - (Children hard non-at-issue) -1.77128 0.210 120.9 -8.437 &lt;.0001 ## (Adults appo at-issue) - (Adults soft non-at-issue) -2.32845 0.172 76.0 -13.563 &lt;.0001 ## (Adults appo at-issue) - (Children soft non-at-issue) -1.70842 0.229 131.6 -7.475 &lt;.0001 ## (Children appo at-issue) - (Adults hard at-issue) 1.15955 0.234 87.1 4.945 0.0002 ## (Children appo at-issue) - (Children hard at-issue) 0.00708 0.176 64.0 0.040 1.0000 ## [ reached getOption(&quot;max.print&quot;) -- omitted 53 rows ] ## ## Degrees-of-freedom method: satterthwaite ## P value adjustment: holm method for 66 tests Questions: Can you think of some reasons why I do not recommend this kind of analysis? 6.5 More emmeans: Interactions There is actually quite a lot that emmeans will let you do. One especially interesting function is illustrated here. For one, we can inspect interactions and check whether lower-order interactions are also present for each level of some factor (here: age group). Again, this is all based on the large model, so we are working with the model estimates rather than the raw data. And again we start with the matrix of effects: # inspection of the interactions for each group separately, excluding NNRCS emm &lt;- emmeans( psp_lmm, ~ trigger_cat * issue, by = c(&quot;stage&quot;) ) emm ## stage = Adults: ## trigger_cat issue emmean SE df lower.CL upper.CL ## appo at-issue 2.03 0.153 78.2 1.73 2.34 ## hard at-issue 2.04 0.160 76.9 1.72 2.35 ## soft at-issue 3.73 0.162 78.6 3.41 4.05 ## appo non-at-issue 4.19 0.134 76.0 3.93 4.46 ## hard non-at-issue 4.51 0.128 75.4 4.25 4.76 ## soft non-at-issue 4.36 0.149 76.6 4.07 4.66 ## ## stage = Children: ## trigger_cat issue emmean SE df lower.CL upper.CL ## appo at-issue 3.20 0.181 85.2 2.83 3.56 ## hard at-issue 3.19 0.188 84.3 2.81 3.56 ## soft at-issue 3.59 0.192 84.5 3.21 3.97 ## appo non-at-issue 3.64 0.160 82.3 3.32 3.96 ## hard non-at-issue 3.80 0.153 83.1 3.50 4.11 ## soft non-at-issue 3.74 0.178 80.9 3.39 4.10 ## ## Degrees-of-freedom method: kenward-roger ## Confidence level used: 0.95 (ints_tests &lt;- joint_tests(emm, by = c(&quot;stage&quot;))) ## stage = Adults: ## model term df1 df2 F.ratio p.value ## trigger_cat 2 45.59 25.760 &lt;.0001 ## issue 1 57.37 330.037 &lt;.0001 ## trigger_cat:issue 2 60.88 55.077 &lt;.0001 ## ## stage = Children: ## model term df1 df2 F.ratio p.value ## trigger_cat 2 60.05 1.285 0.2841 ## issue 1 68.55 11.885 0.0010 ## trigger_cat:issue 2 119.38 2.134 0.1228 Here’s another thing one could do: Because we are not super interested in the non-restrictive relative clauses (they were included as somewhat of a control), we can check what happens to the big interaction when we exclude them. # inspection of the three-way interaction without NRRCs emm_threeway &lt;- emmeans( psp_lmm, ~ stage:trigger_cat:issue, at = list(trigger_cat = c(&quot;soft&quot;, &quot;hard&quot;)) ) (three_way &lt;- joint_tests(emm_threeway)) ## model term df1 df2 F.ratio p.value ## stage 1 58.94 0.221 0.6403 ## trigger_cat 1 46.30 15.706 0.0003 ## issue 1 45.02 105.257 &lt;.0001 ## stage:trigger_cat 1 60.69 11.567 0.0012 ## stage:issue 1 102.34 54.093 &lt;.0001 ## trigger_cat:issue 1 32.54 48.972 &lt;.0001 ## stage:trigger_cat:issue 1 1317.80 28.498 &lt;.0001 6.6 Ordinal data So far, we used linear mixed models for the analysis. However, there are actually good reasons to be cautious here. Recall that in the experiment we used a Likert-type scale with values from 1 to 5: While we pretended that we can use the linear mixed models in this case by treating the ordinal data as real numbers, there are good reasons not to do this. One such reason is that real numbers tend to be equidistant from each other. For ordinal scales, it is not so clear that this is the case. (For all Germans, think about the differences between the grades in school, and especially the distance from 4 to 5.) I will not go into the details of why this is such a problem here for the LMM. For the interested people, there’s the following paper: Liddell and Kruschke (2018). The alternative analysis would require an ordinal (mixed) model, which can be computed in R using the ordinal package (Christensen 2018) with the clmm() (for cumulative link mixed model) function. 6.7 Reporting Here’s a check list of things to report about your statistical analysis (under the assumption that you’re using a mixed model): R plus version and citation Mention how you coded categorical variables (treatment, sum, etc.) Mention fixed and random effects Formula (\\(Y \\sim \\ldots\\)) Table Packages including citation (if not base) and corresponding functions Plot (please mention which error bars you’re using, ideally in the \\(y\\)-axis label) If you want to know how package authors want to have their packages cited, you can do the following and either copy the full citation or copy the entry for you .bib file (if you use some kind of reference manager): citation(&quot;afex&quot;) ## ## To cite package &#39;afex&#39; in publications use: ## ## Singmann H, Bolker B, Westfall J, Aust F, Ben-Shachar M (2022). _afex: Analysis of Factorial ## Experiments_. R package version 1.1-1, &lt;https://CRAN.R-project.org/package=afex&gt;. ## ## A BibTeX entry for LaTeX users is ## ## @Manual{, ## title = {afex: Analysis of Factorial Experiments}, ## author = {Henrik Singmann and Ben Bolker and Jake Westfall and Frederik Aust and Mattan S. Ben-Shachar}, ## year = {2022}, ## note = {R package version 1.1-1}, ## url = {https://CRAN.R-project.org/package=afex}, ## } 6.8 Open questions Anything that you’d like to ask we? 6.9 Fin! Thank you so much! "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
